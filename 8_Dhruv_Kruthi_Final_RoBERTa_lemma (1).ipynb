{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Answering Exploratory Question 3: Does combining deep learning models improve accuracy?"
      ],
      "metadata": {
        "id": "vhQ-SVrqKy4G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nfq9y3MdII_i"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the .npy files\n",
        "X_train = np.load('RoBERTa_train_lemma_embeddings.npy')\n",
        "X_test = np.load('RoBERTa_test_lemma_embeddings.npy')\n",
        "\n",
        "# Optional: check the shape and type\n",
        "print(f\"Train embeddings shape: {X_train.shape}, dtype: {X_train.dtype}\")\n",
        "print(f\"Test embeddings shape: {X_test.shape}, dtype: {X_test.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X9dhNa-JysN",
        "outputId": "26dc046b-1ede-4ce3-aa0f-19aea215fcbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train embeddings shape: (3043, 768), dtype: float32\n",
            "Test embeddings shape: (761, 768), dtype: float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.load('y_train_roberta_lemma.npy')\n",
        "y_test = np.load('y_test_roberta_lemma.npy')\n",
        "\n",
        "print(f\"Train output shape: {y_train.shape}, dtype: {y_train.dtype}\")\n",
        "print(f\"Test output shape: {y_test.shape}, dtype: {y_test.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lURelwS6J8zm",
        "outputId": "9d744b04-1b09-41f1-f201-3b7c7a377d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train output shape: (3043,), dtype: int64\n",
            "Test output shape: (761,), dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize and train the logistic regression model\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nENViP0KSt1",
        "outputId": "a88ef9e9-f2d4-411d-879a-4c696ca5670c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7450722733245729\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.68      0.72       361\n",
            "           1       0.74      0.80      0.77       400\n",
            "\n",
            "    accuracy                           0.75       761\n",
            "   macro avg       0.75      0.74      0.74       761\n",
            "weighted avg       0.75      0.75      0.74       761\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            " [[247 114]\n",
            " [ 80 320]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Step 1: Initialize the SVM model\n",
        "svm_model = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "\n",
        "# Step 2: Fit the model to training data\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 3: Predict on test data\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Step 4: Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDpdOHY-Klzc",
        "outputId": "0ead327a-d6c2-40f0-db6a-69eba9bd3c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7568988173455979\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.70      0.73       361\n",
            "           1       0.75      0.81      0.78       400\n",
            "\n",
            "    accuracy                           0.76       761\n",
            "   macro avg       0.76      0.75      0.75       761\n",
            "weighted avg       0.76      0.76      0.76       761\n",
            "\n",
            "Confusion Matrix:\n",
            " [[254 107]\n",
            " [ 78 322]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Step 1: Initialize the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Step 2: Fit the model to training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 3: Predict on test data\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Step 4: Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SlJZ59PLsqt",
        "outputId": "7b169342-bbe1-4427-e4d8-15ecbb6fa394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7240473061760841\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.63      0.68       361\n",
            "           1       0.71      0.81      0.76       400\n",
            "\n",
            "    accuracy                           0.72       761\n",
            "   macro avg       0.73      0.72      0.72       761\n",
            "weighted avg       0.73      0.72      0.72       761\n",
            "\n",
            "Confusion Matrix:\n",
            " [[227 134]\n",
            " [ 76 324]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "\n",
        "# Step 1: Initialize the XGBoost model\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Step 2: Fit the model to training data\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 3: Predict on test data\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Step 4: Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BrfhTBYL9QO",
        "outputId": "939f0db0-45eb-4ca7-e47f-85a13adfd9d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:18:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7529566360052562\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.68      0.72       361\n",
            "           1       0.74      0.82      0.78       400\n",
            "\n",
            "    accuracy                           0.75       761\n",
            "   macro avg       0.76      0.75      0.75       761\n",
            "weighted avg       0.75      0.75      0.75       761\n",
            "\n",
            "Confusion Matrix:\n",
            " [[246 115]\n",
            " [ 73 327]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "3LsREaIUMHIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 1. GPU Setup\n",
        "# ====================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NULiD_IRN5Ut",
        "outputId": "86636ab0-4dc8-45ad-b2ac-c8074daa3d10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 2. Convert NumPy arrays to PyTorch Tensors\n",
        "# ====================================\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)"
      ],
      "metadata": {
        "id": "kzILpRnFN8AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 3. Create DataLoaders\n",
        "# ====================================\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64)"
      ],
      "metadata": {
        "id": "MwPn0fj_N-j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "10jMgpNGXkze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. LSTM Model (No Dropout)\n",
        "# ====================================\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, lstm_layers=2):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0  # No dropout\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Add seq_len=1 for LSTM: (batch_size, 1, input_dim)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        last_hidden = hn[-1]  # Take last layer's hidden state\n",
        "        logits = self.classifier(last_hidden)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "-bx-1CmmOBH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer\n",
        "# ====================================\n",
        "model = LSTMClassifier(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "KKKjETrWOLZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40 #ACCORDING TO VAL LOSS\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf4TBJJmORX3",
        "outputId": "f2d29396-3397-4fbc-d3ec-4991c114ba5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.2471 - Val Loss: 8.2897\n",
            "Epoch 2/40 - Train Loss: 32.9737 - Val Loss: 8.2331\n",
            "Epoch 3/40 - Train Loss: 29.9022 - Val Loss: 7.4294\n",
            "Epoch 4/40 - Train Loss: 27.2256 - Val Loss: 6.5893\n",
            "Epoch 5/40 - Train Loss: 26.2487 - Val Loss: 6.5032\n",
            "Epoch 6/40 - Train Loss: 25.4152 - Val Loss: 6.7957\n",
            "Epoch 7/40 - Train Loss: 25.6904 - Val Loss: 6.5187\n",
            "Epoch 8/40 - Train Loss: 24.8074 - Val Loss: 6.2627\n",
            "Epoch 9/40 - Train Loss: 24.6659 - Val Loss: 6.5993\n",
            "Epoch 10/40 - Train Loss: 24.1702 - Val Loss: 6.2818\n",
            "Epoch 11/40 - Train Loss: 24.0046 - Val Loss: 6.2105\n",
            "Epoch 12/40 - Train Loss: 23.4935 - Val Loss: 6.2897\n",
            "Epoch 13/40 - Train Loss: 23.3470 - Val Loss: 6.1218\n",
            "Epoch 14/40 - Train Loss: 23.9436 - Val Loss: 6.9036\n",
            "Epoch 15/40 - Train Loss: 23.3874 - Val Loss: 6.1165\n",
            "Epoch 16/40 - Train Loss: 22.8026 - Val Loss: 6.0923\n",
            "Epoch 17/40 - Train Loss: 22.4786 - Val Loss: 6.2441\n",
            "Epoch 18/40 - Train Loss: 22.4921 - Val Loss: 6.2613\n",
            "Epoch 19/40 - Train Loss: 23.1544 - Val Loss: 7.1878\n",
            "Epoch 20/40 - Train Loss: 22.7670 - Val Loss: 6.2232\n",
            "Epoch 21/40 - Train Loss: 21.8014 - Val Loss: 6.3110\n",
            "Epoch 22/40 - Train Loss: 22.0520 - Val Loss: 7.0015\n",
            "Epoch 23/40 - Train Loss: 22.2455 - Val Loss: 6.1406\n",
            "Epoch 24/40 - Train Loss: 21.3624 - Val Loss: 6.5777\n",
            "Epoch 25/40 - Train Loss: 22.6052 - Val Loss: 6.2923\n",
            "Epoch 26/40 - Train Loss: 21.3985 - Val Loss: 6.2927\n",
            "Epoch 27/40 - Train Loss: 20.7250 - Val Loss: 6.3213\n",
            "Epoch 28/40 - Train Loss: 20.3687 - Val Loss: 7.0610\n",
            "Epoch 29/40 - Train Loss: 20.8326 - Val Loss: 6.5011\n",
            "Epoch 30/40 - Train Loss: 20.1678 - Val Loss: 7.6673\n",
            "Epoch 31/40 - Train Loss: 20.5644 - Val Loss: 6.5956\n",
            "Epoch 32/40 - Train Loss: 19.9031 - Val Loss: 6.8430\n",
            "Epoch 33/40 - Train Loss: 20.2611 - Val Loss: 7.1009\n",
            "Epoch 34/40 - Train Loss: 19.7683 - Val Loss: 6.3921\n",
            "Epoch 35/40 - Train Loss: 20.0973 - Val Loss: 7.4049\n",
            "Epoch 36/40 - Train Loss: 20.7570 - Val Loss: 6.3574\n",
            "Epoch 37/40 - Train Loss: 20.1155 - Val Loss: 7.5808\n",
            "Epoch 38/40 - Train Loss: 19.8972 - Val Loss: 6.3258\n",
            "Epoch 39/40 - Train Loss: 19.4035 - Val Loss: 6.3682\n",
            "Epoch 40/40 - Train Loss: 18.5877 - Val Loss: 7.2121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "# Accuracy and Classification Report\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "# Confusion Matrix (numbers only)\n",
        "print(\"ðŸ§® Confusion Matrix:\\n\", confusion_matrix(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPkX6ujBOWN8",
        "outputId": "17601372-0e96-4b2a-9ad9-98fafe3e21b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7543\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7364    0.7507    0.7435       361\n",
            "         1.0     0.7710    0.7575    0.7642       400\n",
            "\n",
            "    accuracy                         0.7543       761\n",
            "   macro avg     0.7537    0.7541    0.7538       761\n",
            "weighted avg     0.7546    0.7543    0.7544       761\n",
            "\n",
            "ðŸ§® Confusion Matrix:\n",
            " [[271  90]\n",
            " [ 97 303]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. Enhanced Bidirectional-LSTM Model (without Dropout)\n",
        "# ====================================\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, lstm_layers=3, bidirectional=True, dropout=0):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional,\n",
        "            dropout=dropout if lstm_layers > 1 else 0  # dropout only applies if num_layers > 1\n",
        "        )\n",
        "\n",
        "        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_output_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch_size, seq_len=1, input_dim)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "\n",
        "        if self.bidirectional:\n",
        "            # Concatenate last forward and backward hidden states\n",
        "            last_hidden = torch.cat((hn[-2], hn[-1]), dim=1)\n",
        "        else:\n",
        "            last_hidden = hn[-1]\n",
        "\n",
        "        logits = self.classifier(last_hidden)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "oLlc-2tAQ2L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer\n",
        "# ====================================\n",
        "model = LSTMClassifier(input_dim=X_train.shape[1]).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "jXFdzIpmPLn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGpqq998ProG",
        "outputId": "398ef979-73d9-4c22-c4d9-d606792ab6b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.1870 - Val Loss: 8.2351\n",
            "Epoch 2/40 - Train Loss: 31.2642 - Val Loss: 7.1709\n",
            "Epoch 3/40 - Train Loss: 30.2460 - Val Loss: 7.1522\n",
            "Epoch 4/40 - Train Loss: 27.4655 - Val Loss: 6.6326\n",
            "Epoch 5/40 - Train Loss: 26.4071 - Val Loss: 7.3475\n",
            "Epoch 6/40 - Train Loss: 26.2919 - Val Loss: 6.3642\n",
            "Epoch 7/40 - Train Loss: 25.3017 - Val Loss: 6.7039\n",
            "Epoch 8/40 - Train Loss: 24.7842 - Val Loss: 7.3729\n",
            "Epoch 9/40 - Train Loss: 24.9695 - Val Loss: 6.2500\n",
            "Epoch 10/40 - Train Loss: 24.6301 - Val Loss: 6.9598\n",
            "Epoch 11/40 - Train Loss: 24.1791 - Val Loss: 6.1490\n",
            "Epoch 12/40 - Train Loss: 24.0524 - Val Loss: 6.2512\n",
            "Epoch 13/40 - Train Loss: 23.3947 - Val Loss: 6.4233\n",
            "Epoch 14/40 - Train Loss: 23.2953 - Val Loss: 6.6704\n",
            "Epoch 15/40 - Train Loss: 24.0422 - Val Loss: 6.0676\n",
            "Epoch 16/40 - Train Loss: 22.7252 - Val Loss: 6.4482\n",
            "Epoch 17/40 - Train Loss: 22.7631 - Val Loss: 6.0920\n",
            "Epoch 18/40 - Train Loss: 22.8568 - Val Loss: 6.1075\n",
            "Epoch 19/40 - Train Loss: 23.0279 - Val Loss: 6.0823\n",
            "Epoch 20/40 - Train Loss: 22.8101 - Val Loss: 6.1478\n",
            "Epoch 21/40 - Train Loss: 22.7933 - Val Loss: 6.1993\n",
            "Epoch 22/40 - Train Loss: 22.1544 - Val Loss: 6.7080\n",
            "Epoch 23/40 - Train Loss: 21.6711 - Val Loss: 6.8936\n",
            "Epoch 24/40 - Train Loss: 21.0176 - Val Loss: 6.7770\n",
            "Epoch 25/40 - Train Loss: 21.8053 - Val Loss: 6.1692\n",
            "Epoch 26/40 - Train Loss: 21.6685 - Val Loss: 6.7118\n",
            "Epoch 27/40 - Train Loss: 20.2853 - Val Loss: 7.0138\n",
            "Epoch 28/40 - Train Loss: 21.7518 - Val Loss: 6.3671\n",
            "Epoch 29/40 - Train Loss: 20.3530 - Val Loss: 6.6316\n",
            "Epoch 30/40 - Train Loss: 20.2777 - Val Loss: 6.3790\n",
            "Epoch 31/40 - Train Loss: 20.3401 - Val Loss: 6.3386\n",
            "Epoch 32/40 - Train Loss: 20.8141 - Val Loss: 6.3248\n",
            "Epoch 33/40 - Train Loss: 19.7739 - Val Loss: 6.3511\n",
            "Epoch 34/40 - Train Loss: 20.9654 - Val Loss: 6.3537\n",
            "Epoch 35/40 - Train Loss: 18.7488 - Val Loss: 6.6770\n",
            "Epoch 36/40 - Train Loss: 19.5618 - Val Loss: 6.3306\n",
            "Epoch 37/40 - Train Loss: 18.1052 - Val Loss: 6.9005\n",
            "Epoch 38/40 - Train Loss: 18.4565 - Val Loss: 7.5047\n",
            "Epoch 39/40 - Train Loss: 18.1447 - Val Loss: 7.8453\n",
            "Epoch 40/40 - Train Loss: 18.2103 - Val Loss: 7.6461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "# Accuracy and Classification Report\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "print(\"\\nðŸ“Œ Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGqKBSYiQFsE",
        "outputId": "d8881f3a-f0b0-4192-f0b1-855288477e18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7661\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7215    0.8255    0.7700       361\n",
            "         1.0     0.8190    0.7125    0.7620       400\n",
            "\n",
            "    accuracy                         0.7661       761\n",
            "   macro avg     0.7703    0.7690    0.7660       761\n",
            "weighted avg     0.7728    0.7661    0.7658       761\n",
            "\n",
            "\n",
            "ðŸ“Œ Confusion Matrix:\n",
            "[[298  63]\n",
            " [115 285]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. LSTM Model (Increased Complexity, No Dropout, Not Bidirectional)\n",
        "# ====================================\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, lstm_layers=3):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0  # No dropout\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch_size, seq_len=1, input_dim)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        last_hidden = hn[-1]  # Take last layer's hidden state\n",
        "        logits = self.classifier(last_hidden)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "1c6wJ4KLQOgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer\n",
        "# ====================================\n",
        "model = LSTMClassifier(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "M3Zn7PNdS3K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJxALUD-S7L_",
        "outputId": "651cd497-7d03-44e2-ed1b-b5aaa4e88809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.2420 - Val Loss: 8.3040\n",
            "Epoch 2/40 - Train Loss: 33.2169 - Val Loss: 8.2947\n",
            "Epoch 3/40 - Train Loss: 31.4607 - Val Loss: 7.4654\n",
            "Epoch 4/40 - Train Loss: 27.8755 - Val Loss: 6.7663\n",
            "Epoch 5/40 - Train Loss: 25.7994 - Val Loss: 6.6108\n",
            "Epoch 6/40 - Train Loss: 25.1818 - Val Loss: 6.4646\n",
            "Epoch 7/40 - Train Loss: 26.7360 - Val Loss: 8.4092\n",
            "Epoch 8/40 - Train Loss: 26.2721 - Val Loss: 6.5695\n",
            "Epoch 9/40 - Train Loss: 24.4999 - Val Loss: 6.3004\n",
            "Epoch 10/40 - Train Loss: 25.4325 - Val Loss: 6.6056\n",
            "Epoch 11/40 - Train Loss: 24.0944 - Val Loss: 6.1773\n",
            "Epoch 12/40 - Train Loss: 24.3076 - Val Loss: 6.1308\n",
            "Epoch 13/40 - Train Loss: 23.5895 - Val Loss: 6.2143\n",
            "Epoch 14/40 - Train Loss: 23.5553 - Val Loss: 6.7992\n",
            "Epoch 15/40 - Train Loss: 23.9165 - Val Loss: 6.4113\n",
            "Epoch 16/40 - Train Loss: 23.4195 - Val Loss: 6.3158\n",
            "Epoch 17/40 - Train Loss: 23.2769 - Val Loss: 6.1628\n",
            "Epoch 18/40 - Train Loss: 23.2450 - Val Loss: 6.1482\n",
            "Epoch 19/40 - Train Loss: 22.9186 - Val Loss: 6.2391\n",
            "Epoch 20/40 - Train Loss: 23.2469 - Val Loss: 6.1044\n",
            "Epoch 21/40 - Train Loss: 22.2754 - Val Loss: 6.3197\n",
            "Epoch 22/40 - Train Loss: 22.9166 - Val Loss: 6.4460\n",
            "Epoch 23/40 - Train Loss: 22.3185 - Val Loss: 6.0527\n",
            "Epoch 24/40 - Train Loss: 22.4863 - Val Loss: 6.1613\n",
            "Epoch 25/40 - Train Loss: 22.2261 - Val Loss: 6.3304\n",
            "Epoch 26/40 - Train Loss: 22.9857 - Val Loss: 6.2388\n",
            "Epoch 27/40 - Train Loss: 21.5444 - Val Loss: 6.3768\n",
            "Epoch 28/40 - Train Loss: 21.3189 - Val Loss: 6.1730\n",
            "Epoch 29/40 - Train Loss: 21.1801 - Val Loss: 6.1709\n",
            "Epoch 30/40 - Train Loss: 20.8980 - Val Loss: 6.4888\n",
            "Epoch 31/40 - Train Loss: 20.9637 - Val Loss: 6.2042\n",
            "Epoch 32/40 - Train Loss: 20.8075 - Val Loss: 7.3623\n",
            "Epoch 33/40 - Train Loss: 20.1538 - Val Loss: 6.9642\n",
            "Epoch 34/40 - Train Loss: 20.6525 - Val Loss: 6.7788\n",
            "Epoch 35/40 - Train Loss: 20.0139 - Val Loss: 6.4500\n",
            "Epoch 36/40 - Train Loss: 19.7803 - Val Loss: 6.3743\n",
            "Epoch 37/40 - Train Loss: 20.6963 - Val Loss: 6.7866\n",
            "Epoch 38/40 - Train Loss: 19.0865 - Val Loss: 6.9842\n",
            "Epoch 39/40 - Train Loss: 18.8706 - Val Loss: 9.1933\n",
            "Epoch 40/40 - Train Loss: 18.8335 - Val Loss: 6.7549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "# Accuracy and Classification Report\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "print(\"\\nðŸ“Œ Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ-KiyjqS9Yn",
        "outputId": "d4294bfd-95d6-4369-ffe6-15afb7bf34cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7608\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7412    0.7618    0.7514       361\n",
            "         1.0     0.7795    0.7600    0.7696       400\n",
            "\n",
            "    accuracy                         0.7608       761\n",
            "   macro avg     0.7604    0.7609    0.7605       761\n",
            "weighted avg     0.7613    0.7608    0.7610       761\n",
            "\n",
            "\n",
            "ðŸ“Œ Confusion Matrix:\n",
            "[[275  86]\n",
            " [ 96 304]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bi-GRU"
      ],
      "metadata": {
        "id": "ha1guaC-XoTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. Bi-GRU Model without Dropout\n",
        "# ====================================\n",
        "class BiGRUClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, gru_layers=2):\n",
        "        super(BiGRUClassifier, self).__init__()\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=gru_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0,  # No dropout\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch_size, seq_len=1, input_dim)\n",
        "        gru_out, _ = self.gru(x)\n",
        "        last_hidden = gru_out[:, -1, :]  # Use last time step output\n",
        "        logits = self.classifier(last_hidden)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "iGai4XNGTKNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer\n",
        "# ====================================\n",
        "model = BiGRUClassifier(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "YhPhy5OabNQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23hGcFU_bTYD",
        "outputId": "d1a4560d-9dd9-4d13-f092-110bca29664c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.1962 - Val Loss: 8.2685\n",
            "Epoch 2/40 - Train Loss: 31.5790 - Val Loss: 8.1433\n",
            "Epoch 3/40 - Train Loss: 27.7871 - Val Loss: 6.7042\n",
            "Epoch 4/40 - Train Loss: 28.0982 - Val Loss: 7.2166\n",
            "Epoch 5/40 - Train Loss: 27.2202 - Val Loss: 7.1406\n",
            "Epoch 6/40 - Train Loss: 26.1921 - Val Loss: 6.4957\n",
            "Epoch 7/40 - Train Loss: 25.2505 - Val Loss: 6.5684\n",
            "Epoch 8/40 - Train Loss: 24.7116 - Val Loss: 6.2812\n",
            "Epoch 9/40 - Train Loss: 24.3306 - Val Loss: 6.3108\n",
            "Epoch 10/40 - Train Loss: 24.4255 - Val Loss: 6.1963\n",
            "Epoch 11/40 - Train Loss: 23.8030 - Val Loss: 6.3291\n",
            "Epoch 12/40 - Train Loss: 24.8819 - Val Loss: 6.8526\n",
            "Epoch 13/40 - Train Loss: 24.9601 - Val Loss: 6.2221\n",
            "Epoch 14/40 - Train Loss: 23.6838 - Val Loss: 6.0938\n",
            "Epoch 15/40 - Train Loss: 23.3214 - Val Loss: 6.0522\n",
            "Epoch 16/40 - Train Loss: 22.8846 - Val Loss: 6.0634\n",
            "Epoch 17/40 - Train Loss: 23.4673 - Val Loss: 6.1039\n",
            "Epoch 18/40 - Train Loss: 22.5001 - Val Loss: 6.5045\n",
            "Epoch 19/40 - Train Loss: 22.9668 - Val Loss: 6.1175\n",
            "Epoch 20/40 - Train Loss: 22.6529 - Val Loss: 6.5269\n",
            "Epoch 21/40 - Train Loss: 22.3618 - Val Loss: 6.0387\n",
            "Epoch 22/40 - Train Loss: 22.6831 - Val Loss: 6.8046\n",
            "Epoch 23/40 - Train Loss: 22.5709 - Val Loss: 6.3851\n",
            "Epoch 24/40 - Train Loss: 21.9611 - Val Loss: 6.1527\n",
            "Epoch 25/40 - Train Loss: 22.4244 - Val Loss: 6.0773\n",
            "Epoch 26/40 - Train Loss: 21.5965 - Val Loss: 6.4503\n",
            "Epoch 27/40 - Train Loss: 21.2527 - Val Loss: 6.2125\n",
            "Epoch 28/40 - Train Loss: 21.6692 - Val Loss: 6.2970\n",
            "Epoch 29/40 - Train Loss: 21.2945 - Val Loss: 6.4494\n",
            "Epoch 30/40 - Train Loss: 22.3480 - Val Loss: 6.9452\n",
            "Epoch 31/40 - Train Loss: 22.8856 - Val Loss: 6.2057\n",
            "Epoch 32/40 - Train Loss: 21.8666 - Val Loss: 6.4618\n",
            "Epoch 33/40 - Train Loss: 20.6466 - Val Loss: 6.6034\n",
            "Epoch 34/40 - Train Loss: 20.1481 - Val Loss: 6.5064\n",
            "Epoch 35/40 - Train Loss: 20.2590 - Val Loss: 6.5332\n",
            "Epoch 36/40 - Train Loss: 19.7117 - Val Loss: 6.9438\n",
            "Epoch 37/40 - Train Loss: 19.6692 - Val Loss: 7.2691\n",
            "Epoch 38/40 - Train Loss: 20.9901 - Val Loss: 6.6185\n",
            "Epoch 39/40 - Train Loss: 19.6983 - Val Loss: 7.1623\n",
            "Epoch 40/40 - Train Loss: 19.1721 - Val Loss: 6.7904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "# Accuracy and Metrics\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nðŸ”¢ Confusion Matrix (TN, FP, FN, TP):\")\n",
        "print(confusion_matrix(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Rq-yuh4bbnI",
        "outputId": "34fbc363-34ab-44ec-c874-9d9019e0a495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7398\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7840    0.6233    0.6944       361\n",
            "         1.0     0.7131    0.8450    0.7735       400\n",
            "\n",
            "    accuracy                         0.7398       761\n",
            "   macro avg     0.7485    0.7341    0.7339       761\n",
            "weighted avg     0.7467    0.7398    0.7360       761\n",
            "\n",
            "\n",
            "ðŸ”¢ Confusion Matrix (TN, FP, FN, TP):\n",
            "[[225 136]\n",
            " [ 62 338]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. Bi-GRU Model without Dropout (Increased Complexity)\n",
        "# ====================================\n",
        "class BiGRUClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, gru_layers=3):\n",
        "        super(BiGRUClassifier, self).__init__()\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=gru_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0,  # No dropout\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 128),  # hidden_dim*2 due to bidirectional\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch_size, seq_len=1, input_dim)\n",
        "        gru_out, _ = self.gru(x)\n",
        "        last_hidden = gru_out[:, -1, :]  # Use last time step output\n",
        "        logits = self.classifier(last_hidden)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "FwXec4oybjKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer\n",
        "# ====================================\n",
        "model = BiGRUClassifier(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "f8X7e1sXce4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD1VhmbtchSO",
        "outputId": "d87904c2-389d-4778-9ff7-eb340916e1ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.2571 - Val Loss: 8.2885\n",
            "Epoch 2/40 - Train Loss: 31.3505 - Val Loss: 7.1270\n",
            "Epoch 3/40 - Train Loss: 27.8711 - Val Loss: 6.5767\n",
            "Epoch 4/40 - Train Loss: 27.3523 - Val Loss: 7.0115\n",
            "Epoch 5/40 - Train Loss: 25.9223 - Val Loss: 6.4252\n",
            "Epoch 6/40 - Train Loss: 25.1090 - Val Loss: 6.2259\n",
            "Epoch 7/40 - Train Loss: 24.5246 - Val Loss: 6.2705\n",
            "Epoch 8/40 - Train Loss: 24.5115 - Val Loss: 6.2209\n",
            "Epoch 9/40 - Train Loss: 23.6601 - Val Loss: 6.6234\n",
            "Epoch 10/40 - Train Loss: 24.0375 - Val Loss: 6.3709\n",
            "Epoch 11/40 - Train Loss: 24.6684 - Val Loss: 7.4220\n",
            "Epoch 12/40 - Train Loss: 24.5052 - Val Loss: 6.0901\n",
            "Epoch 13/40 - Train Loss: 23.5185 - Val Loss: 6.2987\n",
            "Epoch 14/40 - Train Loss: 23.5091 - Val Loss: 6.4686\n",
            "Epoch 15/40 - Train Loss: 23.1917 - Val Loss: 6.0797\n",
            "Epoch 16/40 - Train Loss: 22.6149 - Val Loss: 6.4618\n",
            "Epoch 17/40 - Train Loss: 22.4721 - Val Loss: 6.0429\n",
            "Epoch 18/40 - Train Loss: 23.1963 - Val Loss: 6.2599\n",
            "Epoch 19/40 - Train Loss: 22.9265 - Val Loss: 6.5824\n",
            "Epoch 20/40 - Train Loss: 22.1095 - Val Loss: 6.0071\n",
            "Epoch 21/40 - Train Loss: 22.3069 - Val Loss: 6.0715\n",
            "Epoch 22/40 - Train Loss: 23.1294 - Val Loss: 6.3451\n",
            "Epoch 23/40 - Train Loss: 21.9112 - Val Loss: 6.0975\n",
            "Epoch 24/40 - Train Loss: 21.7792 - Val Loss: 6.3321\n",
            "Epoch 25/40 - Train Loss: 21.8587 - Val Loss: 6.6639\n",
            "Epoch 26/40 - Train Loss: 21.8695 - Val Loss: 6.2522\n",
            "Epoch 27/40 - Train Loss: 21.3589 - Val Loss: 7.0406\n",
            "Epoch 28/40 - Train Loss: 21.4170 - Val Loss: 6.0345\n",
            "Epoch 29/40 - Train Loss: 20.8519 - Val Loss: 6.8385\n",
            "Epoch 30/40 - Train Loss: 20.6307 - Val Loss: 6.3842\n",
            "Epoch 31/40 - Train Loss: 20.7530 - Val Loss: 6.1010\n",
            "Epoch 32/40 - Train Loss: 20.2064 - Val Loss: 6.9219\n",
            "Epoch 33/40 - Train Loss: 20.0260 - Val Loss: 8.0898\n",
            "Epoch 34/40 - Train Loss: 19.5989 - Val Loss: 6.5037\n",
            "Epoch 35/40 - Train Loss: 19.1013 - Val Loss: 6.7957\n",
            "Epoch 36/40 - Train Loss: 19.4329 - Val Loss: 6.6953\n",
            "Epoch 37/40 - Train Loss: 18.3087 - Val Loss: 7.4750\n",
            "Epoch 38/40 - Train Loss: 18.8230 - Val Loss: 6.5523\n",
            "Epoch 39/40 - Train Loss: 18.7322 - Val Loss: 6.9417\n",
            "Epoch 40/40 - Train Loss: 18.1501 - Val Loss: 6.5842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "# Accuracy and Metrics\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nðŸ”¢ Confusion Matrix (TN, FP, FN, TP):\")\n",
        "print(confusion_matrix(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK-cmNr3ckBf",
        "outputId": "bf17f327-2879-43a4-d35d-b97085bfda13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7516\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7240    0.7701    0.7463       361\n",
            "         1.0     0.7798    0.7350    0.7568       400\n",
            "\n",
            "    accuracy                         0.7516       761\n",
            "   macro avg     0.7519    0.7525    0.7515       761\n",
            "weighted avg     0.7533    0.7516    0.7518       761\n",
            "\n",
            "\n",
            "ðŸ”¢ Confusion Matrix (TN, FP, FN, TP):\n",
            "[[278  83]\n",
            " [106 294]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. Bi-GRU Model without Dropout (Even bigger Complexity)\n",
        "# ====================================\n",
        "class BiGRUClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=512, gru_layers=4):\n",
        "        super(BiGRUClassifier, self).__init__()\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=gru_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch_size, seq_len=1, input_dim)\n",
        "        gru_out, _ = self.gru(x)\n",
        "        last_hidden = gru_out[:, -1, :]  # (batch_size, hidden_dim*2)\n",
        "        logits = self.classifier(last_hidden)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "zaPQuXFGdXlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer\n",
        "# ====================================\n",
        "model = BiGRUClassifier(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "RV9ad0jceFRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFUCSAGueHGH",
        "outputId": "dbc1ae85-a192-4e69-85fa-a27c626f92c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.2185 - Val Loss: 8.3070\n",
            "Epoch 2/40 - Train Loss: 30.5597 - Val Loss: 6.8050\n",
            "Epoch 3/40 - Train Loss: 27.9359 - Val Loss: 7.5180\n",
            "Epoch 4/40 - Train Loss: 27.5013 - Val Loss: 6.4529\n",
            "Epoch 5/40 - Train Loss: 25.6038 - Val Loss: 6.2682\n",
            "Epoch 6/40 - Train Loss: 27.0144 - Val Loss: 6.7566\n",
            "Epoch 7/40 - Train Loss: 25.4237 - Val Loss: 6.2983\n",
            "Epoch 8/40 - Train Loss: 26.1289 - Val Loss: 6.7835\n",
            "Epoch 9/40 - Train Loss: 25.0547 - Val Loss: 6.1895\n",
            "Epoch 10/40 - Train Loss: 23.8801 - Val Loss: 6.3427\n",
            "Epoch 11/40 - Train Loss: 23.7844 - Val Loss: 6.2680\n",
            "Epoch 12/40 - Train Loss: 23.9370 - Val Loss: 6.5553\n",
            "Epoch 13/40 - Train Loss: 24.8791 - Val Loss: 6.1053\n",
            "Epoch 14/40 - Train Loss: 24.4110 - Val Loss: 6.7071\n",
            "Epoch 15/40 - Train Loss: 23.6762 - Val Loss: 6.2000\n",
            "Epoch 16/40 - Train Loss: 24.0292 - Val Loss: 6.7128\n",
            "Epoch 17/40 - Train Loss: 23.7449 - Val Loss: 6.3697\n",
            "Epoch 18/40 - Train Loss: 23.5695 - Val Loss: 6.0979\n",
            "Epoch 19/40 - Train Loss: 23.2463 - Val Loss: 6.0674\n",
            "Epoch 20/40 - Train Loss: 22.5099 - Val Loss: 6.0481\n",
            "Epoch 21/40 - Train Loss: 22.2059 - Val Loss: 6.2231\n",
            "Epoch 22/40 - Train Loss: 22.9545 - Val Loss: 6.1000\n",
            "Epoch 23/40 - Train Loss: 22.1589 - Val Loss: 6.2924\n",
            "Epoch 24/40 - Train Loss: 21.6448 - Val Loss: 6.7824\n",
            "Epoch 25/40 - Train Loss: 21.8941 - Val Loss: 6.2004\n",
            "Epoch 26/40 - Train Loss: 21.5243 - Val Loss: 6.2864\n",
            "Epoch 27/40 - Train Loss: 21.5031 - Val Loss: 7.0084\n",
            "Epoch 28/40 - Train Loss: 21.3534 - Val Loss: 6.6411\n",
            "Epoch 29/40 - Train Loss: 21.5927 - Val Loss: 6.2969\n",
            "Epoch 30/40 - Train Loss: 21.8536 - Val Loss: 6.3385\n",
            "Epoch 31/40 - Train Loss: 20.5044 - Val Loss: 6.1923\n",
            "Epoch 32/40 - Train Loss: 19.7270 - Val Loss: 6.7331\n",
            "Epoch 33/40 - Train Loss: 21.2048 - Val Loss: 6.2145\n",
            "Epoch 34/40 - Train Loss: 20.9156 - Val Loss: 6.8774\n",
            "Epoch 35/40 - Train Loss: 20.3798 - Val Loss: 6.2188\n",
            "Epoch 36/40 - Train Loss: 20.0665 - Val Loss: 6.7302\n",
            "Epoch 37/40 - Train Loss: 19.0397 - Val Loss: 6.5617\n",
            "Epoch 38/40 - Train Loss: 20.9553 - Val Loss: 6.5665\n",
            "Epoch 39/40 - Train Loss: 20.8224 - Val Loss: 6.5145\n",
            "Epoch 40/40 - Train Loss: 22.4105 - Val Loss: 6.7957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "# Accuracy and Metrics\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nðŸ”¢ Confusion Matrix (TN, FP, FN, TP):\")\n",
        "print(confusion_matrix(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiIZ9-vAeHoM",
        "outputId": "61fd86c2-e6b6-47ba-e3cb-37130921bdb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7530\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7874    0.6565    0.7160       361\n",
            "         1.0     0.7304    0.8400    0.7814       400\n",
            "\n",
            "    accuracy                         0.7530       761\n",
            "   macro avg     0.7589    0.7483    0.7487       761\n",
            "weighted avg     0.7574    0.7530    0.7504       761\n",
            "\n",
            "\n",
            "ðŸ”¢ Confusion Matrix (TN, FP, FN, TP):\n",
            "[[237 124]\n",
            " [ 64 336]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN"
      ],
      "metadata": {
        "id": "WVvewbU5fGK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. Deep CNN Model (Dropout Removed)\n",
        "# ====================================\n",
        "class DeepCNN(nn.Module):\n",
        "    def __init__(self, input_dim, num_filters=64):\n",
        "        super(DeepCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters * 2, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # Final output size after pooling: 768 â†’ 384 â†’ 192\n",
        "        flattened_dim = num_filters * 2 * 192\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch_size, 1, 768)\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "pH4xp2pIeNXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = DeepCNN(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "ka9QJrJ9jFsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop (No Early Stopping)\n",
        "# ====================================\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYOm6FnIjLjT",
        "outputId": "68e9993f-4e45-465a-c849-fbfb9e0e5f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Train Loss: 33.2842 - Val Loss: 8.3369\n",
            "Epoch 2/100 - Train Loss: 33.2644 - Val Loss: 8.3058\n",
            "Epoch 3/100 - Train Loss: 33.2404 - Val Loss: 8.3012\n",
            "Epoch 4/100 - Train Loss: 33.2296 - Val Loss: 8.3075\n",
            "Epoch 5/100 - Train Loss: 33.2416 - Val Loss: 8.3019\n",
            "Epoch 6/100 - Train Loss: 33.2083 - Val Loss: 8.3006\n",
            "Epoch 7/100 - Train Loss: 33.2382 - Val Loss: 8.3024\n",
            "Epoch 8/100 - Train Loss: 33.2157 - Val Loss: 8.2987\n",
            "Epoch 9/100 - Train Loss: 33.2237 - Val Loss: 8.3045\n",
            "Epoch 10/100 - Train Loss: 33.2424 - Val Loss: 8.3021\n",
            "Epoch 11/100 - Train Loss: 33.2111 - Val Loss: 8.3022\n",
            "Epoch 12/100 - Train Loss: 33.2081 - Val Loss: 8.3007\n",
            "Epoch 13/100 - Train Loss: 33.2075 - Val Loss: 8.2992\n",
            "Epoch 14/100 - Train Loss: 33.1922 - Val Loss: 8.2950\n",
            "Epoch 15/100 - Train Loss: 33.1547 - Val Loss: 8.2729\n",
            "Epoch 16/100 - Train Loss: 32.9946 - Val Loss: 8.2350\n",
            "Epoch 17/100 - Train Loss: 33.2357 - Val Loss: 8.3039\n",
            "Epoch 18/100 - Train Loss: 33.2316 - Val Loss: 8.3021\n",
            "Epoch 19/100 - Train Loss: 33.2101 - Val Loss: 8.3006\n",
            "Epoch 20/100 - Train Loss: 33.0906 - Val Loss: 8.2531\n",
            "Epoch 21/100 - Train Loss: 32.8901 - Val Loss: 8.0716\n",
            "Epoch 22/100 - Train Loss: 32.9879 - Val Loss: 8.2318\n",
            "Epoch 23/100 - Train Loss: 32.8978 - Val Loss: 8.1239\n",
            "Epoch 24/100 - Train Loss: 32.6688 - Val Loss: 8.0604\n",
            "Epoch 25/100 - Train Loss: 32.5785 - Val Loss: 8.0477\n",
            "Epoch 26/100 - Train Loss: 32.4010 - Val Loss: 7.9581\n",
            "Epoch 27/100 - Train Loss: 32.3951 - Val Loss: 8.0295\n",
            "Epoch 28/100 - Train Loss: 32.2414 - Val Loss: 7.9255\n",
            "Epoch 29/100 - Train Loss: 32.1258 - Val Loss: 7.9149\n",
            "Epoch 30/100 - Train Loss: 32.3693 - Val Loss: 8.0255\n",
            "Epoch 31/100 - Train Loss: 32.1865 - Val Loss: 7.8906\n",
            "Epoch 32/100 - Train Loss: 32.1259 - Val Loss: 7.8628\n",
            "Epoch 33/100 - Train Loss: 32.1302 - Val Loss: 7.8529\n",
            "Epoch 34/100 - Train Loss: 32.0006 - Val Loss: 7.8773\n",
            "Epoch 35/100 - Train Loss: 31.9133 - Val Loss: 7.8265\n",
            "Epoch 36/100 - Train Loss: 31.9882 - Val Loss: 7.8192\n",
            "Epoch 37/100 - Train Loss: 31.8435 - Val Loss: 7.7980\n",
            "Epoch 38/100 - Train Loss: 31.8219 - Val Loss: 7.8037\n",
            "Epoch 39/100 - Train Loss: 31.8175 - Val Loss: 7.7908\n",
            "Epoch 40/100 - Train Loss: 31.6541 - Val Loss: 7.7616\n",
            "Epoch 41/100 - Train Loss: 31.5485 - Val Loss: 7.6897\n",
            "Epoch 42/100 - Train Loss: 30.8162 - Val Loss: 7.7652\n",
            "Epoch 43/100 - Train Loss: 31.4986 - Val Loss: 7.8857\n",
            "Epoch 44/100 - Train Loss: 29.9333 - Val Loss: 7.0381\n",
            "Epoch 45/100 - Train Loss: 29.5248 - Val Loss: 6.9737\n",
            "Epoch 46/100 - Train Loss: 28.5960 - Val Loss: 7.0438\n",
            "Epoch 47/100 - Train Loss: 28.2600 - Val Loss: 6.8481\n",
            "Epoch 48/100 - Train Loss: 28.1818 - Val Loss: 6.9346\n",
            "Epoch 49/100 - Train Loss: 28.7673 - Val Loss: 6.7979\n",
            "Epoch 50/100 - Train Loss: 27.9290 - Val Loss: 6.8259\n",
            "Epoch 51/100 - Train Loss: 27.9937 - Val Loss: 6.7970\n",
            "Epoch 52/100 - Train Loss: 27.6042 - Val Loss: 6.7006\n",
            "Epoch 53/100 - Train Loss: 27.0152 - Val Loss: 6.7044\n",
            "Epoch 54/100 - Train Loss: 26.8335 - Val Loss: 6.6435\n",
            "Epoch 55/100 - Train Loss: 27.0497 - Val Loss: 6.6397\n",
            "Epoch 56/100 - Train Loss: 26.6297 - Val Loss: 6.6160\n",
            "Epoch 57/100 - Train Loss: 26.5493 - Val Loss: 6.5944\n",
            "Epoch 58/100 - Train Loss: 26.3381 - Val Loss: 6.5714\n",
            "Epoch 59/100 - Train Loss: 26.9669 - Val Loss: 7.1220\n",
            "Epoch 60/100 - Train Loss: 26.8564 - Val Loss: 6.6841\n",
            "Epoch 61/100 - Train Loss: 26.5726 - Val Loss: 6.7152\n",
            "Epoch 62/100 - Train Loss: 26.1257 - Val Loss: 6.5294\n",
            "Epoch 63/100 - Train Loss: 26.0984 - Val Loss: 6.5622\n",
            "Epoch 64/100 - Train Loss: 26.0627 - Val Loss: 6.6016\n",
            "Epoch 65/100 - Train Loss: 26.0446 - Val Loss: 6.5359\n",
            "Epoch 66/100 - Train Loss: 25.8048 - Val Loss: 6.5604\n",
            "Epoch 67/100 - Train Loss: 25.7090 - Val Loss: 6.5554\n",
            "Epoch 68/100 - Train Loss: 25.8662 - Val Loss: 6.4544\n",
            "Epoch 69/100 - Train Loss: 25.5225 - Val Loss: 6.5360\n",
            "Epoch 70/100 - Train Loss: 25.5420 - Val Loss: 6.6308\n",
            "Epoch 71/100 - Train Loss: 25.8469 - Val Loss: 6.4285\n",
            "Epoch 72/100 - Train Loss: 25.9395 - Val Loss: 7.0231\n",
            "Epoch 73/100 - Train Loss: 26.0985 - Val Loss: 6.5055\n",
            "Epoch 74/100 - Train Loss: 25.5529 - Val Loss: 6.8351\n",
            "Epoch 75/100 - Train Loss: 25.3436 - Val Loss: 6.6447\n",
            "Epoch 76/100 - Train Loss: 25.2889 - Val Loss: 6.4385\n",
            "Epoch 77/100 - Train Loss: 25.6572 - Val Loss: 6.4734\n",
            "Epoch 78/100 - Train Loss: 25.0228 - Val Loss: 6.5330\n",
            "Epoch 79/100 - Train Loss: 25.1782 - Val Loss: 6.4078\n",
            "Epoch 80/100 - Train Loss: 25.1628 - Val Loss: 6.4130\n",
            "Epoch 81/100 - Train Loss: 24.9404 - Val Loss: 6.3724\n",
            "Epoch 82/100 - Train Loss: 24.9453 - Val Loss: 6.3903\n",
            "Epoch 83/100 - Train Loss: 25.2472 - Val Loss: 6.6897\n",
            "Epoch 84/100 - Train Loss: 24.8010 - Val Loss: 6.4694\n",
            "Epoch 85/100 - Train Loss: 24.8727 - Val Loss: 6.7572\n",
            "Epoch 86/100 - Train Loss: 25.0776 - Val Loss: 6.6773\n",
            "Epoch 87/100 - Train Loss: 24.4889 - Val Loss: 6.3817\n",
            "Epoch 88/100 - Train Loss: 24.6788 - Val Loss: 6.6050\n",
            "Epoch 89/100 - Train Loss: 24.8081 - Val Loss: 6.4790\n",
            "Epoch 90/100 - Train Loss: 24.4125 - Val Loss: 6.6608\n",
            "Epoch 91/100 - Train Loss: 24.6617 - Val Loss: 6.4758\n",
            "Epoch 92/100 - Train Loss: 24.5425 - Val Loss: 6.3641\n",
            "Epoch 93/100 - Train Loss: 24.4030 - Val Loss: 6.7267\n",
            "Epoch 94/100 - Train Loss: 25.2401 - Val Loss: 6.4733\n",
            "Epoch 95/100 - Train Loss: 24.6817 - Val Loss: 6.6513\n",
            "Epoch 96/100 - Train Loss: 24.3552 - Val Loss: 6.3738\n",
            "Epoch 97/100 - Train Loss: 24.3363 - Val Loss: 6.3888\n",
            "Epoch 98/100 - Train Loss: 24.1413 - Val Loss: 6.3660\n",
            "Epoch 99/100 - Train Loss: 24.2078 - Val Loss: 6.4231\n",
            "Epoch 100/100 - Train Loss: 24.2299 - Val Loss: 6.3712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "# Accuracy and Classification Report\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"\\nðŸ”¢ Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kmjy4EqjWSj",
        "outputId": "7f0021d3-95d4-4266-e973-125a90e7764b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7477\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7649    0.6759    0.7176       361\n",
            "         1.0     0.7353    0.8125    0.7720       400\n",
            "\n",
            "    accuracy                         0.7477       761\n",
            "   macro avg     0.7501    0.7442    0.7448       761\n",
            "weighted avg     0.7493    0.7477    0.7462       761\n",
            "\n",
            "\n",
            "ðŸ”¢ Confusion Matrix:\n",
            "[[244 117]\n",
            " [ 75 325]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. Deep More Complex CNN Model (Dropout Removed)\n",
        "# ====================================\n",
        "class ComplexCNN(nn.Module):\n",
        "    def __init__(self, input_dim, num_filters=128):\n",
        "        super(ComplexCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, num_filters, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(num_filters)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(num_filters, num_filters * 2, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(num_filters * 2)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(num_filters * 2, num_filters * 4, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(num_filters * 4)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # 768 â†’ 384 â†’ 192 â†’ 96\n",
        "        self.flattened_dim = num_filters * 4 * 96\n",
        "\n",
        "        self.fc1 = nn.Linear(self.flattened_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ],
      "metadata": {
        "id": "ykyDlv5Ij70y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = DeepCNN(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "jIuG9HTClgev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop (No Early Stopping)\n",
        "# ====================================\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avt00Inwli_8",
        "outputId": "98265e50-22f6-409a-ad5d-f74b37067ca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Train Loss: 33.2855 - Val Loss: 8.3038\n",
            "Epoch 2/100 - Train Loss: 33.2103 - Val Loss: 8.3016\n",
            "Epoch 3/100 - Train Loss: 33.2075 - Val Loss: 8.3009\n",
            "Epoch 4/100 - Train Loss: 33.2176 - Val Loss: 8.3010\n",
            "Epoch 5/100 - Train Loss: 33.2219 - Val Loss: 8.3023\n",
            "Epoch 6/100 - Train Loss: 33.2130 - Val Loss: 8.3005\n",
            "Epoch 7/100 - Train Loss: 33.2149 - Val Loss: 8.3026\n",
            "Epoch 8/100 - Train Loss: 33.2243 - Val Loss: 8.3004\n",
            "Epoch 9/100 - Train Loss: 33.1992 - Val Loss: 8.2969\n",
            "Epoch 10/100 - Train Loss: 33.1866 - Val Loss: 8.2842\n",
            "Epoch 11/100 - Train Loss: 33.1552 - Val Loss: 8.2932\n",
            "Epoch 12/100 - Train Loss: 33.1430 - Val Loss: 8.2729\n",
            "Epoch 13/100 - Train Loss: 32.8834 - Val Loss: 8.4158\n",
            "Epoch 14/100 - Train Loss: 32.4891 - Val Loss: 8.5977\n",
            "Epoch 15/100 - Train Loss: 31.7114 - Val Loss: 7.5740\n",
            "Epoch 16/100 - Train Loss: 30.2332 - Val Loss: 7.3369\n",
            "Epoch 17/100 - Train Loss: 29.9016 - Val Loss: 7.4932\n",
            "Epoch 18/100 - Train Loss: 28.7181 - Val Loss: 7.0954\n",
            "Epoch 19/100 - Train Loss: 31.1472 - Val Loss: 7.3866\n",
            "Epoch 20/100 - Train Loss: 28.9283 - Val Loss: 7.2511\n",
            "Epoch 21/100 - Train Loss: 28.6683 - Val Loss: 7.0017\n",
            "Epoch 22/100 - Train Loss: 28.2647 - Val Loss: 6.9585\n",
            "Epoch 23/100 - Train Loss: 28.3240 - Val Loss: 7.1331\n",
            "Epoch 24/100 - Train Loss: 27.9529 - Val Loss: 6.9703\n",
            "Epoch 25/100 - Train Loss: 27.5890 - Val Loss: 6.9760\n",
            "Epoch 26/100 - Train Loss: 28.0921 - Val Loss: 7.2147\n",
            "Epoch 27/100 - Train Loss: 28.0402 - Val Loss: 7.2457\n",
            "Epoch 28/100 - Train Loss: 28.2501 - Val Loss: 6.8638\n",
            "Epoch 29/100 - Train Loss: 27.7097 - Val Loss: 7.1051\n",
            "Epoch 30/100 - Train Loss: 28.1828 - Val Loss: 6.8476\n",
            "Epoch 31/100 - Train Loss: 27.1985 - Val Loss: 6.8464\n",
            "Epoch 32/100 - Train Loss: 27.5307 - Val Loss: 6.8135\n",
            "Epoch 33/100 - Train Loss: 27.1600 - Val Loss: 6.7864\n",
            "Epoch 34/100 - Train Loss: 27.1835 - Val Loss: 7.1019\n",
            "Epoch 35/100 - Train Loss: 27.4173 - Val Loss: 6.7561\n",
            "Epoch 36/100 - Train Loss: 26.8096 - Val Loss: 6.7191\n",
            "Epoch 37/100 - Train Loss: 26.8127 - Val Loss: 6.7275\n",
            "Epoch 38/100 - Train Loss: 26.4441 - Val Loss: 6.7647\n",
            "Epoch 39/100 - Train Loss: 26.1662 - Val Loss: 6.7181\n",
            "Epoch 40/100 - Train Loss: 26.0418 - Val Loss: 6.6672\n",
            "Epoch 41/100 - Train Loss: 26.2973 - Val Loss: 6.6599\n",
            "Epoch 42/100 - Train Loss: 25.9871 - Val Loss: 6.6538\n",
            "Epoch 43/100 - Train Loss: 26.0110 - Val Loss: 6.5788\n",
            "Epoch 44/100 - Train Loss: 25.4874 - Val Loss: 6.6410\n",
            "Epoch 45/100 - Train Loss: 25.2234 - Val Loss: 6.6080\n",
            "Epoch 46/100 - Train Loss: 25.3953 - Val Loss: 6.5907\n",
            "Epoch 47/100 - Train Loss: 25.1204 - Val Loss: 6.5002\n",
            "Epoch 48/100 - Train Loss: 24.5856 - Val Loss: 6.6247\n",
            "Epoch 49/100 - Train Loss: 25.1208 - Val Loss: 7.0482\n",
            "Epoch 50/100 - Train Loss: 25.6876 - Val Loss: 6.8588\n",
            "Epoch 51/100 - Train Loss: 24.2617 - Val Loss: 6.7292\n",
            "Epoch 52/100 - Train Loss: 24.3582 - Val Loss: 7.0242\n",
            "Epoch 53/100 - Train Loss: 24.1472 - Val Loss: 6.6068\n",
            "Epoch 54/100 - Train Loss: 24.2453 - Val Loss: 6.4940\n",
            "Epoch 55/100 - Train Loss: 24.3325 - Val Loss: 6.5811\n",
            "Epoch 56/100 - Train Loss: 24.1031 - Val Loss: 6.7342\n",
            "Epoch 57/100 - Train Loss: 23.6344 - Val Loss: 6.6198\n",
            "Epoch 58/100 - Train Loss: 23.5817 - Val Loss: 6.5705\n",
            "Epoch 59/100 - Train Loss: 23.8760 - Val Loss: 6.6491\n",
            "Epoch 60/100 - Train Loss: 23.4399 - Val Loss: 6.9242\n",
            "Epoch 61/100 - Train Loss: 23.4281 - Val Loss: 6.5137\n",
            "Epoch 62/100 - Train Loss: 23.3264 - Val Loss: 6.6901\n",
            "Epoch 63/100 - Train Loss: 23.3255 - Val Loss: 6.6758\n",
            "Epoch 64/100 - Train Loss: 23.1381 - Val Loss: 6.6106\n",
            "Epoch 65/100 - Train Loss: 22.5789 - Val Loss: 6.6189\n",
            "Epoch 66/100 - Train Loss: 22.7519 - Val Loss: 6.6251\n",
            "Epoch 67/100 - Train Loss: 23.0312 - Val Loss: 6.5565\n",
            "Epoch 68/100 - Train Loss: 22.3294 - Val Loss: 6.7368\n",
            "Epoch 69/100 - Train Loss: 22.1345 - Val Loss: 6.7886\n",
            "Epoch 70/100 - Train Loss: 21.8880 - Val Loss: 6.6671\n",
            "Epoch 71/100 - Train Loss: 22.5772 - Val Loss: 6.5598\n",
            "Epoch 72/100 - Train Loss: 22.0637 - Val Loss: 6.6087\n",
            "Epoch 73/100 - Train Loss: 21.6218 - Val Loss: 6.6725\n",
            "Epoch 74/100 - Train Loss: 22.0124 - Val Loss: 6.8219\n",
            "Epoch 75/100 - Train Loss: 21.9891 - Val Loss: 6.6804\n",
            "Epoch 76/100 - Train Loss: 21.0214 - Val Loss: 6.7168\n",
            "Epoch 77/100 - Train Loss: 21.1880 - Val Loss: 6.6811\n",
            "Epoch 78/100 - Train Loss: 21.4722 - Val Loss: 6.9005\n",
            "Epoch 79/100 - Train Loss: 21.0704 - Val Loss: 7.0842\n",
            "Epoch 80/100 - Train Loss: 20.8171 - Val Loss: 7.0509\n",
            "Epoch 81/100 - Train Loss: 20.3676 - Val Loss: 6.8796\n",
            "Epoch 82/100 - Train Loss: 20.8068 - Val Loss: 6.6747\n",
            "Epoch 83/100 - Train Loss: 20.4750 - Val Loss: 6.8480\n",
            "Epoch 84/100 - Train Loss: 20.1200 - Val Loss: 7.1162\n",
            "Epoch 85/100 - Train Loss: 20.4489 - Val Loss: 7.0392\n",
            "Epoch 86/100 - Train Loss: 20.1831 - Val Loss: 6.7409\n",
            "Epoch 87/100 - Train Loss: 20.1738 - Val Loss: 6.8552\n",
            "Epoch 88/100 - Train Loss: 19.1244 - Val Loss: 7.1637\n",
            "Epoch 89/100 - Train Loss: 19.8269 - Val Loss: 6.9285\n",
            "Epoch 90/100 - Train Loss: 19.0613 - Val Loss: 7.0971\n",
            "Epoch 91/100 - Train Loss: 19.0513 - Val Loss: 7.1376\n",
            "Epoch 92/100 - Train Loss: 19.8188 - Val Loss: 7.2390\n",
            "Epoch 93/100 - Train Loss: 18.8397 - Val Loss: 7.3467\n",
            "Epoch 94/100 - Train Loss: 18.6877 - Val Loss: 7.1340\n",
            "Epoch 95/100 - Train Loss: 19.0828 - Val Loss: 6.9476\n",
            "Epoch 96/100 - Train Loss: 18.5853 - Val Loss: 8.0126\n",
            "Epoch 97/100 - Train Loss: 18.3803 - Val Loss: 7.4876\n",
            "Epoch 98/100 - Train Loss: 18.1519 - Val Loss: 7.9229\n",
            "Epoch 99/100 - Train Loss: 18.2669 - Val Loss: 7.2020\n",
            "Epoch 100/100 - Train Loss: 18.5893 - Val Loss: 7.1679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "# Accuracy and Classification Report\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"\\nðŸ”¢ Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJGIntGxllki",
        "outputId": "0e24b445-ee4c-420d-980d-77d6e9188f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7240\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7649    0.6039    0.6749       361\n",
            "         1.0     0.6996    0.8325    0.7603       400\n",
            "\n",
            "    accuracy                         0.7240       761\n",
            "   macro avg     0.7322    0.7182    0.7176       761\n",
            "weighted avg     0.7306    0.7240    0.7198       761\n",
            "\n",
            "\n",
            "ðŸ”¢ Confusion Matrix:\n",
            "[[218 143]\n",
            " [ 67 333]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. Deep Most Complex CNN Model (Dropout Removed)\n",
        "# ====================================\n",
        "class ComplexCNN(nn.Module):\n",
        "    def __init__(self, input_dim, num_filters=128):\n",
        "        super(ComplexCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=num_filters, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=num_filters, out_channels=num_filters * 2, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(in_channels=num_filters * 2, out_channels=num_filters * 4, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        self.conv4 = nn.Conv1d(in_channels=num_filters * 4, out_channels=num_filters * 8, kernel_size=3, padding=1)\n",
        "        self.pool4 = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        # Final pooled size after 4 maxpools: 768 â†’ 384 â†’ 192 â†’ 96 â†’ 48\n",
        "        self.flattened_dim = num_filters * 8 * 48  # 1024 * 48 = 49,152\n",
        "\n",
        "        # Updated classifier with 4 layers: 256 â†’ 128 â†’ 64 â†’ 1\n",
        "        self.fc1 = nn.Linear(self.flattened_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch_size, 1, 768)\n",
        "\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))  # (batch_size, 128, 384)\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))  # (batch_size, 256, 192)\n",
        "        x = self.pool3(torch.relu(self.conv3(x)))  # (batch_size, 512, 96)\n",
        "        x = self.pool4(torch.relu(self.conv4(x)))  # (batch_size, 1024, 48)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten â†’ (batch_size, 1024 * 48)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        return self.fc4(x)"
      ],
      "metadata": {
        "id": "RjrpWweSlse1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = DeepCNN(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "-nzSuY6dmlNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop (No Early Stopping)\n",
        "# ====================================\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG6k-vvom1Tv",
        "outputId": "42e81dbc-43ef-41b3-b389-27aad25f1a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Train Loss: 33.3095 - Val Loss: 8.3010\n",
            "Epoch 2/100 - Train Loss: 33.2177 - Val Loss: 8.3069\n",
            "Epoch 3/100 - Train Loss: 33.2415 - Val Loss: 8.2995\n",
            "Epoch 4/100 - Train Loss: 33.2267 - Val Loss: 8.3004\n",
            "Epoch 5/100 - Train Loss: 33.2279 - Val Loss: 8.3007\n",
            "Epoch 6/100 - Train Loss: 33.2203 - Val Loss: 8.2993\n",
            "Epoch 7/100 - Train Loss: 33.2233 - Val Loss: 8.3002\n",
            "Epoch 8/100 - Train Loss: 33.2197 - Val Loss: 8.2959\n",
            "Epoch 9/100 - Train Loss: 33.1828 - Val Loss: 8.2908\n",
            "Epoch 10/100 - Train Loss: 33.1829 - Val Loss: 8.2787\n",
            "Epoch 11/100 - Train Loss: 33.1234 - Val Loss: 8.2403\n",
            "Epoch 12/100 - Train Loss: 32.9684 - Val Loss: 8.1559\n",
            "Epoch 13/100 - Train Loss: 32.6443 - Val Loss: 8.1617\n",
            "Epoch 14/100 - Train Loss: 32.4537 - Val Loss: 7.9735\n",
            "Epoch 15/100 - Train Loss: 32.2128 - Val Loss: 7.9621\n",
            "Epoch 16/100 - Train Loss: 31.7016 - Val Loss: 7.9367\n",
            "Epoch 17/100 - Train Loss: 30.7342 - Val Loss: 7.5321\n",
            "Epoch 18/100 - Train Loss: 30.1551 - Val Loss: 7.3799\n",
            "Epoch 19/100 - Train Loss: 29.3764 - Val Loss: 7.6837\n",
            "Epoch 20/100 - Train Loss: 28.7031 - Val Loss: 7.2682\n",
            "Epoch 21/100 - Train Loss: 28.3780 - Val Loss: 7.1615\n",
            "Epoch 22/100 - Train Loss: 27.8956 - Val Loss: 7.2724\n",
            "Epoch 23/100 - Train Loss: 27.5332 - Val Loss: 7.0364\n",
            "Epoch 24/100 - Train Loss: 27.8188 - Val Loss: 7.0086\n",
            "Epoch 25/100 - Train Loss: 27.9699 - Val Loss: 7.0741\n",
            "Epoch 26/100 - Train Loss: 27.2701 - Val Loss: 7.0341\n",
            "Epoch 27/100 - Train Loss: 27.5562 - Val Loss: 6.9782\n",
            "Epoch 28/100 - Train Loss: 26.9039 - Val Loss: 6.9362\n",
            "Epoch 29/100 - Train Loss: 26.8751 - Val Loss: 6.8866\n",
            "Epoch 30/100 - Train Loss: 26.4565 - Val Loss: 6.8765\n",
            "Epoch 31/100 - Train Loss: 26.3761 - Val Loss: 6.9266\n",
            "Epoch 32/100 - Train Loss: 26.6781 - Val Loss: 6.8915\n",
            "Epoch 33/100 - Train Loss: 26.4183 - Val Loss: 6.8396\n",
            "Epoch 34/100 - Train Loss: 26.4693 - Val Loss: 6.8832\n",
            "Epoch 35/100 - Train Loss: 26.7967 - Val Loss: 6.9998\n",
            "Epoch 36/100 - Train Loss: 26.5018 - Val Loss: 6.8056\n",
            "Epoch 37/100 - Train Loss: 26.3482 - Val Loss: 6.8166\n",
            "Epoch 38/100 - Train Loss: 26.3259 - Val Loss: 6.8363\n",
            "Epoch 39/100 - Train Loss: 26.0160 - Val Loss: 6.9154\n",
            "Epoch 40/100 - Train Loss: 26.0530 - Val Loss: 6.7885\n",
            "Epoch 41/100 - Train Loss: 26.2306 - Val Loss: 6.8729\n",
            "Epoch 42/100 - Train Loss: 26.1715 - Val Loss: 7.9553\n",
            "Epoch 43/100 - Train Loss: 26.5689 - Val Loss: 6.8819\n",
            "Epoch 44/100 - Train Loss: 26.0362 - Val Loss: 6.8161\n",
            "Epoch 45/100 - Train Loss: 25.7899 - Val Loss: 6.7506\n",
            "Epoch 46/100 - Train Loss: 26.1184 - Val Loss: 6.7401\n",
            "Epoch 47/100 - Train Loss: 25.7293 - Val Loss: 6.7369\n",
            "Epoch 48/100 - Train Loss: 26.2616 - Val Loss: 6.7350\n",
            "Epoch 49/100 - Train Loss: 25.8443 - Val Loss: 6.8073\n",
            "Epoch 50/100 - Train Loss: 25.7549 - Val Loss: 6.7054\n",
            "Epoch 51/100 - Train Loss: 25.7010 - Val Loss: 6.8560\n",
            "Epoch 52/100 - Train Loss: 26.1479 - Val Loss: 6.6907\n",
            "Epoch 53/100 - Train Loss: 25.9233 - Val Loss: 6.8871\n",
            "Epoch 54/100 - Train Loss: 25.6822 - Val Loss: 6.6766\n",
            "Epoch 55/100 - Train Loss: 25.7873 - Val Loss: 6.6892\n",
            "Epoch 56/100 - Train Loss: 25.4568 - Val Loss: 6.7266\n",
            "Epoch 57/100 - Train Loss: 25.9314 - Val Loss: 6.8172\n",
            "Epoch 58/100 - Train Loss: 25.5583 - Val Loss: 6.7576\n",
            "Epoch 59/100 - Train Loss: 25.5712 - Val Loss: 6.6475\n",
            "Epoch 60/100 - Train Loss: 25.2919 - Val Loss: 6.7382\n",
            "Epoch 61/100 - Train Loss: 25.2620 - Val Loss: 6.8229\n",
            "Epoch 62/100 - Train Loss: 25.5920 - Val Loss: 6.7833\n",
            "Epoch 63/100 - Train Loss: 25.3573 - Val Loss: 6.6327\n",
            "Epoch 64/100 - Train Loss: 25.2830 - Val Loss: 6.6157\n",
            "Epoch 65/100 - Train Loss: 25.2184 - Val Loss: 6.7073\n",
            "Epoch 66/100 - Train Loss: 25.4078 - Val Loss: 6.6132\n",
            "Epoch 67/100 - Train Loss: 25.2272 - Val Loss: 6.7220\n",
            "Epoch 68/100 - Train Loss: 25.3087 - Val Loss: 6.6793\n",
            "Epoch 69/100 - Train Loss: 24.7182 - Val Loss: 6.5232\n",
            "Epoch 70/100 - Train Loss: 24.7528 - Val Loss: 6.5927\n",
            "Epoch 71/100 - Train Loss: 24.7348 - Val Loss: 6.6075\n",
            "Epoch 72/100 - Train Loss: 24.9703 - Val Loss: 6.6457\n",
            "Epoch 73/100 - Train Loss: 25.1623 - Val Loss: 6.6865\n",
            "Epoch 74/100 - Train Loss: 25.1720 - Val Loss: 6.7177\n",
            "Epoch 75/100 - Train Loss: 24.2608 - Val Loss: 6.5601\n",
            "Epoch 76/100 - Train Loss: 24.2841 - Val Loss: 6.6191\n",
            "Epoch 77/100 - Train Loss: 24.5061 - Val Loss: 6.4508\n",
            "Epoch 78/100 - Train Loss: 24.3289 - Val Loss: 7.2480\n",
            "Epoch 79/100 - Train Loss: 23.9964 - Val Loss: 7.0038\n",
            "Epoch 80/100 - Train Loss: 24.2022 - Val Loss: 6.6457\n",
            "Epoch 81/100 - Train Loss: 23.9952 - Val Loss: 6.4403\n",
            "Epoch 82/100 - Train Loss: 23.7525 - Val Loss: 6.4745\n",
            "Epoch 83/100 - Train Loss: 23.7611 - Val Loss: 6.6915\n",
            "Epoch 84/100 - Train Loss: 23.0319 - Val Loss: 6.9197\n",
            "Epoch 85/100 - Train Loss: 23.1810 - Val Loss: 6.5216\n",
            "Epoch 86/100 - Train Loss: 22.9373 - Val Loss: 6.4336\n",
            "Epoch 87/100 - Train Loss: 22.8051 - Val Loss: 6.4806\n",
            "Epoch 88/100 - Train Loss: 22.3888 - Val Loss: 6.6261\n",
            "Epoch 89/100 - Train Loss: 22.9621 - Val Loss: 6.9186\n",
            "Epoch 90/100 - Train Loss: 22.1892 - Val Loss: 6.3395\n",
            "Epoch 91/100 - Train Loss: 22.1984 - Val Loss: 6.5416\n",
            "Epoch 92/100 - Train Loss: 21.8097 - Val Loss: 6.6991\n",
            "Epoch 93/100 - Train Loss: 22.1381 - Val Loss: 6.5621\n",
            "Epoch 94/100 - Train Loss: 21.6426 - Val Loss: 6.4835\n",
            "Epoch 95/100 - Train Loss: 21.6885 - Val Loss: 6.7550\n",
            "Epoch 96/100 - Train Loss: 22.0160 - Val Loss: 6.7179\n",
            "Epoch 97/100 - Train Loss: 21.6152 - Val Loss: 6.5534\n",
            "Epoch 98/100 - Train Loss: 21.4055 - Val Loss: 6.4364\n",
            "Epoch 99/100 - Train Loss: 21.6648 - Val Loss: 6.3285\n",
            "Epoch 100/100 - Train Loss: 20.6923 - Val Loss: 6.7452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "# Accuracy and Classification Report\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"\\nðŸ”¢ Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbXlz5Jkm4PV",
        "outputId": "8de820b6-a7c1-4775-9f30-68fd32bd502a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7490\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7457    0.7147    0.7298       361\n",
            "         1.0     0.7518    0.7800    0.7656       400\n",
            "\n",
            "    accuracy                         0.7490       761\n",
            "   macro avg     0.7487    0.7473    0.7477       761\n",
            "weighted avg     0.7489    0.7490    0.7487       761\n",
            "\n",
            "\n",
            "ðŸ”¢ Confusion Matrix:\n",
            "[[258 103]\n",
            " [ 88 312]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bi-GRU_Bi-LSTM"
      ],
      "metadata": {
        "id": "QCMNA22csD-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. Parallel BiGRU_BiLSTM Model\n",
        "# ====================================\n",
        "class BiGRU_BiLSTM_Parallel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, gru_layers=3, lstm_layers=3, dropout=0):\n",
        "        super(BiGRU_BiLSTM_Parallel, self).__init__()\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=gru_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # New 4-layer classifier: 256 â†’ 128 â†’ 64 â†’ 1\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 4, 256),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(dropout),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(dropout),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(dropout),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch_size, 1, input_dim)\n",
        "\n",
        "        # Bi-GRU\n",
        "        gru_out, _ = self.gru(x)\n",
        "        gru_last = gru_out[:, -1, :]  # (batch_size, hidden_dim*2)\n",
        "\n",
        "        # Bi-LSTM\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        lstm_last = torch.cat((hn[-2], hn[-1]), dim=1)  # (batch_size, hidden_dim*2)\n",
        "\n",
        "        # Concatenate features\n",
        "        combined = torch.cat((gru_last, lstm_last), dim=1)  # (batch_size, hidden_dim*4)\n",
        "        return self.classifier(combined)\n"
      ],
      "metadata": {
        "id": "eiUl_Vy2m6dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = BiGRU_BiLSTM_Parallel(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "3r_wnXw9sMJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsYtvQplsru8",
        "outputId": "ff268b55-a1f3-4ad1-a363-fd24e2fd6f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.2304 - Val Loss: 8.2995\n",
            "Epoch 2/40 - Train Loss: 31.5951 - Val Loss: 7.2203\n",
            "Epoch 3/40 - Train Loss: 29.0437 - Val Loss: 6.7606\n",
            "Epoch 4/40 - Train Loss: 28.0518 - Val Loss: 7.1685\n",
            "Epoch 5/40 - Train Loss: 26.3176 - Val Loss: 6.7287\n",
            "Epoch 6/40 - Train Loss: 25.2643 - Val Loss: 6.7744\n",
            "Epoch 7/40 - Train Loss: 24.8976 - Val Loss: 6.3516\n",
            "Epoch 8/40 - Train Loss: 24.4320 - Val Loss: 6.2208\n",
            "Epoch 9/40 - Train Loss: 23.9904 - Val Loss: 6.2430\n",
            "Epoch 10/40 - Train Loss: 24.5398 - Val Loss: 6.2858\n",
            "Epoch 11/40 - Train Loss: 24.6053 - Val Loss: 6.1375\n",
            "Epoch 12/40 - Train Loss: 23.3206 - Val Loss: 6.1044\n",
            "Epoch 13/40 - Train Loss: 22.9929 - Val Loss: 6.3819\n",
            "Epoch 14/40 - Train Loss: 23.4977 - Val Loss: 6.3444\n",
            "Epoch 15/40 - Train Loss: 24.0822 - Val Loss: 6.8483\n",
            "Epoch 16/40 - Train Loss: 23.9227 - Val Loss: 6.0586\n",
            "Epoch 17/40 - Train Loss: 22.8826 - Val Loss: 6.3635\n",
            "Epoch 18/40 - Train Loss: 22.2282 - Val Loss: 6.1599\n",
            "Epoch 19/40 - Train Loss: 22.4037 - Val Loss: 6.0863\n",
            "Epoch 20/40 - Train Loss: 22.5239 - Val Loss: 6.0978\n",
            "Epoch 21/40 - Train Loss: 21.9329 - Val Loss: 6.2255\n",
            "Epoch 22/40 - Train Loss: 22.4171 - Val Loss: 6.1790\n",
            "Epoch 23/40 - Train Loss: 22.0857 - Val Loss: 6.3319\n",
            "Epoch 24/40 - Train Loss: 21.8252 - Val Loss: 6.9361\n",
            "Epoch 25/40 - Train Loss: 21.3744 - Val Loss: 6.4687\n",
            "Epoch 26/40 - Train Loss: 21.4235 - Val Loss: 6.4387\n",
            "Epoch 27/40 - Train Loss: 21.0689 - Val Loss: 6.1326\n",
            "Epoch 28/40 - Train Loss: 21.3018 - Val Loss: 6.6332\n",
            "Epoch 29/40 - Train Loss: 20.3593 - Val Loss: 7.0519\n",
            "Epoch 30/40 - Train Loss: 20.2723 - Val Loss: 6.5945\n",
            "Epoch 31/40 - Train Loss: 22.4646 - Val Loss: 6.8669\n",
            "Epoch 32/40 - Train Loss: 20.2339 - Val Loss: 6.1820\n",
            "Epoch 33/40 - Train Loss: 20.2673 - Val Loss: 7.4749\n",
            "Epoch 34/40 - Train Loss: 19.9558 - Val Loss: 6.6790\n",
            "Epoch 35/40 - Train Loss: 19.3528 - Val Loss: 6.7893\n",
            "Epoch 36/40 - Train Loss: 18.8370 - Val Loss: 7.0361\n",
            "Epoch 37/40 - Train Loss: 20.1335 - Val Loss: 6.5409\n",
            "Epoch 38/40 - Train Loss: 18.7953 - Val Loss: 7.3214\n",
            "Epoch 39/40 - Train Loss: 19.1032 - Val Loss: 6.3906\n",
            "Epoch 40/40 - Train Loss: 18.8578 - Val Loss: 7.1559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "# Accuracy and Classification Report\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"\\nðŸ”¢ Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7aHYy2fsweU",
        "outputId": "41c17761-c12b-4cf4-ae65-aeb2d6bc4d9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7622\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7586    0.7313    0.7447       361\n",
            "         1.0     0.7651    0.7900    0.7774       400\n",
            "\n",
            "    accuracy                         0.7622       761\n",
            "   macro avg     0.7619    0.7607    0.7610       761\n",
            "weighted avg     0.7620    0.7622    0.7619       761\n",
            "\n",
            "\n",
            "ðŸ”¢ Confusion Matrix:\n",
            "[[264  97]\n",
            " [ 84 316]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. Sequential BiGRU_BiLSTM Model\n",
        "# ====================================\n",
        "class BiGRU_BiLSTM_Sequential(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=256, lstm_layers=3, gru_layers=3, dropout=0):\n",
        "        super(BiGRU_BiLSTM_Sequential, self).__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=hidden_dim * 2,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=gru_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0\n",
        "        )\n",
        "\n",
        "        # Updated classifier: 256 â†’ 128 â†’ 64 â†’ 1\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch_size, 1, input_dim)\n",
        "\n",
        "        # Bi-LSTM\n",
        "        lstm_out, _ = self.lstm(x)  # (batch_size, 1, hidden_dim*2)\n",
        "\n",
        "        # Bi-GRU\n",
        "        gru_out, _ = self.gru(lstm_out)  # (batch_size, 1, hidden_dim*2)\n",
        "        last_hidden = gru_out[:, -1, :]  # final time step\n",
        "\n",
        "        return self.classifier(last_hidden)"
      ],
      "metadata": {
        "id": "mYVxpyZMs-FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = BiGRU_BiLSTM_Sequential(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "Z2HuT6KDtvI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFzaDehYt3mJ",
        "outputId": "8cdc8b54-390f-4994-ff24-6e1dddb69ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.2896 - Val Loss: 8.3014\n",
            "Epoch 2/40 - Train Loss: 32.0881 - Val Loss: 8.0753\n",
            "Epoch 3/40 - Train Loss: 28.9337 - Val Loss: 7.3529\n",
            "Epoch 4/40 - Train Loss: 27.4819 - Val Loss: 6.6251\n",
            "Epoch 5/40 - Train Loss: 25.9906 - Val Loss: 6.3661\n",
            "Epoch 6/40 - Train Loss: 25.3483 - Val Loss: 6.6626\n",
            "Epoch 7/40 - Train Loss: 25.5725 - Val Loss: 6.3587\n",
            "Epoch 8/40 - Train Loss: 24.6842 - Val Loss: 7.0272\n",
            "Epoch 9/40 - Train Loss: 24.9046 - Val Loss: 6.5959\n",
            "Epoch 10/40 - Train Loss: 24.2967 - Val Loss: 6.2266\n",
            "Epoch 11/40 - Train Loss: 24.2360 - Val Loss: 6.0794\n",
            "Epoch 12/40 - Train Loss: 23.8795 - Val Loss: 6.3349\n",
            "Epoch 13/40 - Train Loss: 23.4626 - Val Loss: 6.2322\n",
            "Epoch 14/40 - Train Loss: 23.6709 - Val Loss: 6.1734\n",
            "Epoch 15/40 - Train Loss: 23.1576 - Val Loss: 6.2809\n",
            "Epoch 16/40 - Train Loss: 23.1662 - Val Loss: 6.0638\n",
            "Epoch 17/40 - Train Loss: 22.3335 - Val Loss: 6.2042\n",
            "Epoch 18/40 - Train Loss: 22.7249 - Val Loss: 6.2159\n",
            "Epoch 19/40 - Train Loss: 22.1104 - Val Loss: 6.0965\n",
            "Epoch 20/40 - Train Loss: 21.8098 - Val Loss: 6.1138\n",
            "Epoch 21/40 - Train Loss: 21.3748 - Val Loss: 6.2617\n",
            "Epoch 22/40 - Train Loss: 22.2525 - Val Loss: 6.3371\n",
            "Epoch 23/40 - Train Loss: 21.9222 - Val Loss: 6.0757\n",
            "Epoch 24/40 - Train Loss: 21.9217 - Val Loss: 6.4617\n",
            "Epoch 25/40 - Train Loss: 23.3580 - Val Loss: 6.4758\n",
            "Epoch 26/40 - Train Loss: 21.2527 - Val Loss: 6.0945\n",
            "Epoch 27/40 - Train Loss: 20.5818 - Val Loss: 6.5488\n",
            "Epoch 28/40 - Train Loss: 20.7847 - Val Loss: 7.2919\n",
            "Epoch 29/40 - Train Loss: 20.4669 - Val Loss: 7.5461\n",
            "Epoch 30/40 - Train Loss: 20.7092 - Val Loss: 6.1694\n",
            "Epoch 31/40 - Train Loss: 21.4359 - Val Loss: 6.4168\n",
            "Epoch 32/40 - Train Loss: 19.1791 - Val Loss: 6.5344\n",
            "Epoch 33/40 - Train Loss: 19.0783 - Val Loss: 6.6420\n",
            "Epoch 34/40 - Train Loss: 18.8233 - Val Loss: 6.7109\n",
            "Epoch 35/40 - Train Loss: 18.9234 - Val Loss: 6.2535\n",
            "Epoch 36/40 - Train Loss: 17.8185 - Val Loss: 6.5713\n",
            "Epoch 37/40 - Train Loss: 17.9152 - Val Loss: 7.4783\n",
            "Epoch 38/40 - Train Loss: 17.1638 - Val Loss: 6.6781\n",
            "Epoch 39/40 - Train Loss: 16.5894 - Val Loss: 7.1259\n",
            "Epoch 40/40 - Train Loss: 18.5039 - Val Loss: 6.1396\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "# Accuracy and Classification Report\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"\\nðŸ”¢ Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13L5lU_ot9f_",
        "outputId": "67379b29-5a7d-4884-f47f-6d96757eed1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7516\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.6784    0.9058    0.7758       361\n",
            "         1.0     0.8781    0.6125    0.7216       400\n",
            "\n",
            "    accuracy                         0.7516       761\n",
            "   macro avg     0.7783    0.7592    0.7487       761\n",
            "weighted avg     0.7834    0.7516    0.7473       761\n",
            "\n",
            "\n",
            "ðŸ”¢ Confusion Matrix:\n",
            "[[327  34]\n",
            " [155 245]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bi-GRU_CNN"
      ],
      "metadata": {
        "id": "v91whYPuxHRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. Parallel CNN_BiGRU Model\n",
        "# ====================================\n",
        "class CNN_BiGRU_Parallel(nn.Module):\n",
        "    def __init__(self, input_dim=768, gru_hidden_dim=256, gru_layers=3, cnn_filters=128):\n",
        "        super(CNN_BiGRU_Parallel, self).__init__()\n",
        "\n",
        "        # GRU branch\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=gru_hidden_dim,\n",
        "            num_layers=gru_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # CNN branch\n",
        "        self.conv1 = nn.Conv1d(1, cnn_filters, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(cnn_filters)\n",
        "        self.pool1 = nn.MaxPool1d(2)  # 768 â†’ 384\n",
        "\n",
        "        self.conv2 = nn.Conv1d(cnn_filters, cnn_filters * 2, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(cnn_filters * 2)\n",
        "        self.pool2 = nn.MaxPool1d(2)  # 384 â†’ 192\n",
        "\n",
        "        self.conv3 = nn.Conv1d(cnn_filters * 2, cnn_filters * 4, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(cnn_filters * 4)\n",
        "        self.pool3 = nn.MaxPool1d(2)  # 192 â†’ 96\n",
        "\n",
        "        self.conv4 = nn.Conv1d(cnn_filters * 4, cnn_filters * 8, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm1d(cnn_filters * 8)\n",
        "        self.pool4 = nn.MaxPool1d(2)  # 96 â†’ 48\n",
        "\n",
        "        self.cnn_out_dim = cnn_filters * 8 * 48  # 1024 * 48 = 49,152\n",
        "\n",
        "        # Updated combined classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(gru_hidden_dim * 2 + self.cnn_out_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # GRU branch\n",
        "        x_gru = x.unsqueeze(1)  # (batch, 1, input_dim)\n",
        "        gru_out, _ = self.gru(x_gru)\n",
        "        gru_feat = gru_out[:, -1, :]  # (batch, 2 * hidden_dim)\n",
        "\n",
        "        # CNN branch\n",
        "        x_cnn = x.unsqueeze(1)  # (batch, 1, input_dim)\n",
        "        x_cnn = self.pool1(torch.relu(self.bn1(self.conv1(x_cnn))))  # â†’ 384\n",
        "        x_cnn = self.pool2(torch.relu(self.bn2(self.conv2(x_cnn))))  # â†’ 192\n",
        "        x_cnn = self.pool3(torch.relu(self.bn3(self.conv3(x_cnn))))  # â†’ 96\n",
        "        x_cnn = self.pool4(torch.relu(self.bn4(self.conv4(x_cnn))))  # â†’ 48\n",
        "        cnn_feat = x_cnn.view(x_cnn.size(0), -1)  # Flatten\n",
        "\n",
        "        # Concatenate features\n",
        "        combined = torch.cat((gru_feat, cnn_feat), dim=1)\n",
        "        return self.classifier(combined)"
      ],
      "metadata": {
        "id": "bsgmpcdXuHIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = CNN_BiGRU_Parallel(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "J4Mg-gh6xO04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40 # ONLY FOR THIS CASE\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVhLSBw2xUsp",
        "outputId": "fe427be3-763b-4884-d7cf-2b65890cc6c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 34.4042 - Val Loss: 8.3038\n",
            "Epoch 2/40 - Train Loss: 33.4290 - Val Loss: 8.2682\n",
            "Epoch 3/40 - Train Loss: 30.5878 - Val Loss: 7.2945\n",
            "Epoch 4/40 - Train Loss: 27.8898 - Val Loss: 6.8401\n",
            "Epoch 5/40 - Train Loss: 26.4114 - Val Loss: 6.4651\n",
            "Epoch 6/40 - Train Loss: 26.2243 - Val Loss: 6.4404\n",
            "Epoch 7/40 - Train Loss: 25.3284 - Val Loss: 6.6064\n",
            "Epoch 8/40 - Train Loss: 24.7127 - Val Loss: 6.5571\n",
            "Epoch 9/40 - Train Loss: 24.3219 - Val Loss: 6.2282\n",
            "Epoch 10/40 - Train Loss: 23.9955 - Val Loss: 6.1382\n",
            "Epoch 11/40 - Train Loss: 23.4451 - Val Loss: 6.1713\n",
            "Epoch 12/40 - Train Loss: 24.4223 - Val Loss: 6.2283\n",
            "Epoch 13/40 - Train Loss: 24.2688 - Val Loss: 6.6038\n",
            "Epoch 14/40 - Train Loss: 23.9493 - Val Loss: 7.0729\n",
            "Epoch 15/40 - Train Loss: 24.1780 - Val Loss: 6.4055\n",
            "Epoch 16/40 - Train Loss: 23.4161 - Val Loss: 6.7043\n",
            "Epoch 17/40 - Train Loss: 23.2478 - Val Loss: 6.3722\n",
            "Epoch 18/40 - Train Loss: 22.6447 - Val Loss: 6.1274\n",
            "Epoch 19/40 - Train Loss: 22.4852 - Val Loss: 6.2991\n",
            "Epoch 20/40 - Train Loss: 22.4317 - Val Loss: 6.4104\n",
            "Epoch 21/40 - Train Loss: 21.6067 - Val Loss: 6.5060\n",
            "Epoch 22/40 - Train Loss: 21.9324 - Val Loss: 6.4712\n",
            "Epoch 23/40 - Train Loss: 21.6769 - Val Loss: 6.4123\n",
            "Epoch 24/40 - Train Loss: 22.1304 - Val Loss: 6.4086\n",
            "Epoch 25/40 - Train Loss: 21.6646 - Val Loss: 6.5478\n",
            "Epoch 26/40 - Train Loss: 20.8910 - Val Loss: 6.6045\n",
            "Epoch 27/40 - Train Loss: 20.4106 - Val Loss: 6.4589\n",
            "Epoch 28/40 - Train Loss: 20.1593 - Val Loss: 6.8457\n",
            "Epoch 29/40 - Train Loss: 19.9754 - Val Loss: 7.5299\n",
            "Epoch 30/40 - Train Loss: 23.7389 - Val Loss: 6.5034\n",
            "Epoch 31/40 - Train Loss: 20.5848 - Val Loss: 6.6269\n",
            "Epoch 32/40 - Train Loss: 20.2585 - Val Loss: 7.0039\n",
            "Epoch 33/40 - Train Loss: 20.2815 - Val Loss: 7.2737\n",
            "Epoch 34/40 - Train Loss: 19.8308 - Val Loss: 6.6142\n",
            "Epoch 35/40 - Train Loss: 19.7920 - Val Loss: 6.6711\n",
            "Epoch 36/40 - Train Loss: 20.8394 - Val Loss: 6.4978\n",
            "Epoch 37/40 - Train Loss: 20.5797 - Val Loss: 6.1978\n",
            "Epoch 38/40 - Train Loss: 18.7532 - Val Loss: 7.1217\n",
            "Epoch 39/40 - Train Loss: 20.6466 - Val Loss: 6.9041\n",
            "Epoch 40/40 - Train Loss: 18.7650 - Val Loss: 7.5720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "print(\"\\nðŸ”¢ Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9tsxBKOxapH",
        "outputId": "f2c5e0d1-59fe-41eb-891d-9f3c9a9ac40b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7503\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7232    0.7673    0.7446       361\n",
            "         1.0     0.7778    0.7350    0.7558       400\n",
            "\n",
            "    accuracy                         0.7503       761\n",
            "   macro avg     0.7505    0.7512    0.7502       761\n",
            "weighted avg     0.7519    0.7503    0.7505       761\n",
            "\n",
            "\n",
            "ðŸ”¢ Confusion Matrix:\n",
            "[[277  84]\n",
            " [106 294]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. Sequential CNN_BiGRU Model\n",
        "# ====================================\n",
        "class CNN_BiGRU_Sequential(nn.Module):\n",
        "    def __init__(self, input_dim=768, cnn_filters=128, gru_hidden_dim=256, gru_layers=3):\n",
        "        super(CNN_BiGRU_Sequential, self).__init__()\n",
        "\n",
        "        # 4-layer CNN stack\n",
        "        self.conv1 = nn.Conv1d(1, cnn_filters, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(cnn_filters)\n",
        "        self.pool1 = nn.MaxPool1d(2)  # 768 â†’ 384\n",
        "\n",
        "        self.conv2 = nn.Conv1d(cnn_filters, cnn_filters * 2, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(cnn_filters * 2)\n",
        "        self.pool2 = nn.MaxPool1d(2)  # 384 â†’ 192\n",
        "\n",
        "        self.conv3 = nn.Conv1d(cnn_filters * 2, cnn_filters * 4, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(cnn_filters * 4)\n",
        "        self.pool3 = nn.MaxPool1d(2)  # 192 â†’ 96\n",
        "\n",
        "        self.conv4 = nn.Conv1d(cnn_filters * 4, cnn_filters * 8, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm1d(cnn_filters * 8)\n",
        "        self.pool4 = nn.MaxPool1d(2)  # 96 â†’ 48\n",
        "\n",
        "        self.cnn_out_channels = cnn_filters * 8  # 1024\n",
        "        self.cnn_seq_len = 48  # after 4 poolings\n",
        "        self.gru_input_dim = self.cnn_out_channels\n",
        "\n",
        "        # Bi-GRU\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=self.gru_input_dim,\n",
        "            hidden_size=gru_hidden_dim,\n",
        "            num_layers=gru_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Updated classifier: 256 â†’ 128 â†’ 64 â†’ 1\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(gru_hidden_dim * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch, 1, 768)\n",
        "\n",
        "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool4(torch.relu(self.bn4(self.conv4(x))))  # (batch, 1024, 48)\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # (batch, seq_len=48, feature_dim=1024)\n",
        "        gru_out, _ = self.gru(x)\n",
        "        last_hidden = gru_out[:, -1, :]  # (batch, 2*hidden_dim)\n",
        "\n",
        "        return self.classifier(last_hidden)"
      ],
      "metadata": {
        "id": "2XRf7POjyfJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = CNN_BiGRU_Sequential(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "KFnOz5g80DzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFM7Ec5N0JdH",
        "outputId": "cb4d4662-335d-46ae-dc2d-abb033ab1806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.2710 - Val Loss: 8.3029\n",
            "Epoch 2/40 - Train Loss: 33.2614 - Val Loss: 8.3047\n",
            "Epoch 3/40 - Train Loss: 33.2292 - Val Loss: 8.3027\n",
            "Epoch 4/40 - Train Loss: 33.2205 - Val Loss: 8.3033\n",
            "Epoch 5/40 - Train Loss: 33.2227 - Val Loss: 8.3020\n",
            "Epoch 6/40 - Train Loss: 33.2293 - Val Loss: 8.3027\n",
            "Epoch 7/40 - Train Loss: 33.2202 - Val Loss: 8.3020\n",
            "Epoch 8/40 - Train Loss: 33.2087 - Val Loss: 8.3020\n",
            "Epoch 9/40 - Train Loss: 33.2097 - Val Loss: 8.3021\n",
            "Epoch 10/40 - Train Loss: 33.2161 - Val Loss: 8.3021\n",
            "Epoch 11/40 - Train Loss: 33.2095 - Val Loss: 8.3020\n",
            "Epoch 12/40 - Train Loss: 33.2073 - Val Loss: 8.3021\n",
            "Epoch 13/40 - Train Loss: 33.2190 - Val Loss: 8.3020\n",
            "Epoch 14/40 - Train Loss: 33.2067 - Val Loss: 8.3020\n",
            "Epoch 15/40 - Train Loss: 33.2099 - Val Loss: 8.3020\n",
            "Epoch 16/40 - Train Loss: 33.2204 - Val Loss: 8.3020\n",
            "Epoch 17/40 - Train Loss: 33.2157 - Val Loss: 8.3020\n",
            "Epoch 18/40 - Train Loss: 33.2114 - Val Loss: 8.3021\n",
            "Epoch 19/40 - Train Loss: 33.2058 - Val Loss: 8.3020\n",
            "Epoch 20/40 - Train Loss: 33.2075 - Val Loss: 8.3020\n",
            "Epoch 21/40 - Train Loss: 33.2116 - Val Loss: 8.3022\n",
            "Epoch 22/40 - Train Loss: 33.2111 - Val Loss: 8.3021\n",
            "Epoch 23/40 - Train Loss: 33.2083 - Val Loss: 8.3020\n",
            "Epoch 24/40 - Train Loss: 33.2061 - Val Loss: 8.3020\n",
            "Epoch 25/40 - Train Loss: 33.2144 - Val Loss: 8.3020\n",
            "Epoch 26/40 - Train Loss: 33.2051 - Val Loss: 8.3020\n",
            "Epoch 27/40 - Train Loss: 33.2085 - Val Loss: 8.3020\n",
            "Epoch 28/40 - Train Loss: 33.2102 - Val Loss: 8.3020\n",
            "Epoch 29/40 - Train Loss: 33.2050 - Val Loss: 8.3020\n",
            "Epoch 30/40 - Train Loss: 33.2112 - Val Loss: 8.3020\n",
            "Epoch 31/40 - Train Loss: 33.2110 - Val Loss: 8.3020\n",
            "Epoch 32/40 - Train Loss: 33.2151 - Val Loss: 8.3020\n",
            "Epoch 33/40 - Train Loss: 33.2087 - Val Loss: 8.3020\n",
            "Epoch 34/40 - Train Loss: 33.2083 - Val Loss: 8.3020\n",
            "Epoch 35/40 - Train Loss: 33.2055 - Val Loss: 8.3020\n",
            "Epoch 36/40 - Train Loss: 33.2134 - Val Loss: 8.3020\n",
            "Epoch 37/40 - Train Loss: 33.2111 - Val Loss: 8.3020\n",
            "Epoch 38/40 - Train Loss: 33.2050 - Val Loss: 8.3020\n",
            "Epoch 39/40 - Train Loss: 33.2058 - Val Loss: 8.3020\n",
            "Epoch 40/40 - Train Loss: 33.2021 - Val Loss: 8.3020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "print(\"\\nðŸ”¢ Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvFH9Nr10LWR",
        "outputId": "75f7e9aa-b549-4648-d7cb-400d53987275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.5256\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.0000    0.0000    0.0000       361\n",
            "         1.0     0.5256    1.0000    0.6891       400\n",
            "\n",
            "    accuracy                         0.5256       761\n",
            "   macro avg     0.2628    0.5000    0.3445       761\n",
            "weighted avg     0.2763    0.5256    0.3622       761\n",
            "\n",
            "\n",
            "ðŸ”¢ Confusion Matrix:\n",
            "[[  0 361]\n",
            " [  0 400]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. CNN_BiGRU_Interleaved Model\n",
        "# ====================================\n",
        "class CNN_BiGRU_Interleaved(nn.Module):\n",
        "    def __init__(self, input_dim=768, cnn_filters=128, gru_hidden=256, gru_layers=1):\n",
        "        super(CNN_BiGRU_Interleaved, self).__init__()\n",
        "\n",
        "        # Block 1: CNN âž BiGRU\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(1, cnn_filters, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_filters),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2)  # 768 â†’ 384\n",
        "        )\n",
        "        self.bi_gru1 = nn.GRU(\n",
        "            input_size=cnn_filters,\n",
        "            hidden_size=gru_hidden,\n",
        "            num_layers=gru_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Block 2: CNN âž BiGRU\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv1d(gru_hidden * 2, cnn_filters * 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_filters * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2)  # 384 â†’ 192\n",
        "        )\n",
        "        self.bi_gru2 = nn.GRU(\n",
        "            input_size=cnn_filters * 2,\n",
        "            hidden_size=gru_hidden,\n",
        "            num_layers=gru_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Block 3: CNN âž BiGRU\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv1d(gru_hidden * 2, cnn_filters * 4, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_filters * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2)  # 192 â†’ 96\n",
        "        )\n",
        "        self.bi_gru3 = nn.GRU(\n",
        "            input_size=cnn_filters * 4,\n",
        "            hidden_size=gru_hidden,\n",
        "            num_layers=gru_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Final classifier: 256 â†’ 128 â†’ 64 â†’ 1\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(gru_hidden * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # (batch, 1, 768)\n",
        "\n",
        "        # Block 1\n",
        "        x = self.conv1(x)             # (batch, cnn_filters, 384)\n",
        "        x = x.permute(0, 2, 1)        # (batch, 384, cnn_filters)\n",
        "        x, _ = self.bi_gru1(x)        # (batch, 384, gru_hidden*2)\n",
        "\n",
        "        # Block 2\n",
        "        x = x.permute(0, 2, 1)        # (batch, gru_hidden*2, 384)\n",
        "        x = self.conv2(x)             # (batch, cnn_filters*2, 192)\n",
        "        x = x.permute(0, 2, 1)        # (batch, 192, cnn_filters*2)\n",
        "        x, _ = self.bi_gru2(x)        # (batch, 192, gru_hidden*2)\n",
        "\n",
        "        # Block 3\n",
        "        x = x.permute(0, 2, 1)        # (batch, gru_hidden*2, 192)\n",
        "        x = self.conv3(x)             # (batch, cnn_filters*4, 96)\n",
        "        x = x.permute(0, 2, 1)        # (batch, 96, cnn_filters*4)\n",
        "        x, _ = self.bi_gru3(x)        # (batch, 96, gru_hidden*2)\n",
        "\n",
        "        x = x[:, -1, :]               # Last time step â†’ (batch, gru_hidden*2)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "UosGdr0Q0ZIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = CNN_BiGRU_Interleaved(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "_Oq8oiuZ3DjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40 # JUST FOR THIS\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04aadZV83Vuk",
        "outputId": "f9e94edb-76cc-416e-c201-451cbdb5c3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.2008 - Val Loss: 8.2996\n",
            "Epoch 2/40 - Train Loss: 32.8110 - Val Loss: 8.0812\n",
            "Epoch 3/40 - Train Loss: 31.8001 - Val Loss: 8.3616\n",
            "Epoch 4/40 - Train Loss: 31.5223 - Val Loss: 8.2458\n",
            "Epoch 5/40 - Train Loss: 31.6596 - Val Loss: 7.9130\n",
            "Epoch 6/40 - Train Loss: 30.6054 - Val Loss: 7.9160\n",
            "Epoch 7/40 - Train Loss: 30.6874 - Val Loss: 7.8738\n",
            "Epoch 8/40 - Train Loss: 29.9703 - Val Loss: 7.8177\n",
            "Epoch 9/40 - Train Loss: 29.3996 - Val Loss: 7.8848\n",
            "Epoch 10/40 - Train Loss: 29.0273 - Val Loss: 7.3796\n",
            "Epoch 11/40 - Train Loss: 28.5408 - Val Loss: 7.7049\n",
            "Epoch 12/40 - Train Loss: 28.2048 - Val Loss: 7.3271\n",
            "Epoch 13/40 - Train Loss: 27.3619 - Val Loss: 7.6130\n",
            "Epoch 14/40 - Train Loss: 27.4654 - Val Loss: 6.9656\n",
            "Epoch 15/40 - Train Loss: 26.0507 - Val Loss: 7.4080\n",
            "Epoch 16/40 - Train Loss: 25.5515 - Val Loss: 7.1057\n",
            "Epoch 17/40 - Train Loss: 24.5948 - Val Loss: 7.4424\n",
            "Epoch 18/40 - Train Loss: 23.1695 - Val Loss: 9.0689\n",
            "Epoch 19/40 - Train Loss: 22.2581 - Val Loss: 6.6496\n",
            "Epoch 20/40 - Train Loss: 21.1775 - Val Loss: 6.4784\n",
            "Epoch 21/40 - Train Loss: 21.1343 - Val Loss: 6.9325\n",
            "Epoch 22/40 - Train Loss: 17.7433 - Val Loss: 7.6079\n",
            "Epoch 23/40 - Train Loss: 16.8170 - Val Loss: 8.0956\n",
            "Epoch 24/40 - Train Loss: 15.3523 - Val Loss: 7.3215\n",
            "Epoch 25/40 - Train Loss: 13.0182 - Val Loss: 13.4038\n",
            "Epoch 26/40 - Train Loss: 10.7065 - Val Loss: 12.1186\n",
            "Epoch 27/40 - Train Loss: 9.7556 - Val Loss: 8.9380\n",
            "Epoch 28/40 - Train Loss: 7.8721 - Val Loss: 11.2632\n",
            "Epoch 29/40 - Train Loss: 6.5520 - Val Loss: 11.4365\n",
            "Epoch 30/40 - Train Loss: 5.2054 - Val Loss: 13.3363\n",
            "Epoch 31/40 - Train Loss: 4.2424 - Val Loss: 15.8026\n",
            "Epoch 32/40 - Train Loss: 4.9808 - Val Loss: 12.6623\n",
            "Epoch 33/40 - Train Loss: 2.8305 - Val Loss: 16.9623\n",
            "Epoch 34/40 - Train Loss: 2.5260 - Val Loss: 13.9368\n",
            "Epoch 35/40 - Train Loss: 1.6318 - Val Loss: 15.8227\n",
            "Epoch 36/40 - Train Loss: 2.6725 - Val Loss: 15.4198\n",
            "Epoch 37/40 - Train Loss: 4.0780 - Val Loss: 16.5779\n",
            "Epoch 38/40 - Train Loss: 1.3433 - Val Loss: 18.0918\n",
            "Epoch 39/40 - Train Loss: 0.5054 - Val Loss: 20.7346\n",
            "Epoch 40/40 - Train Loss: 1.5449 - Val Loss: 15.1062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "print(\"\\nðŸ”¢ Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppPag3cV3bLh",
        "outputId": "6488445a-15be-46a5-dad4-c0e523f161b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7227\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7404    0.6399    0.6865       361\n",
            "         1.0     0.7105    0.7975    0.7515       400\n",
            "\n",
            "    accuracy                         0.7227       761\n",
            "   macro avg     0.7254    0.7187    0.7190       761\n",
            "weighted avg     0.7247    0.7227    0.7206       761\n",
            "\n",
            "\n",
            "ðŸ”¢ Confusion Matrix:\n",
            "[[231 130]\n",
            " [ 81 319]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN-LSTM"
      ],
      "metadata": {
        "id": "_4H0QhGj4sR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. CNN_LSTM_Parallel Model\n",
        "# ====================================\n",
        "class CNN_LSTM_Parallel(nn.Module):\n",
        "    def __init__(self, input_dim=768, cnn_filters=128, lstm_hidden=256, lstm_layers=3, dropout=0):\n",
        "        super(CNN_LSTM_Parallel, self).__init__()\n",
        "\n",
        "        # CNN Branch\n",
        "        self.conv1 = nn.Conv1d(1, cnn_filters, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(cnn_filters)\n",
        "        self.pool1 = nn.MaxPool1d(2)  # 768 â†’ 384\n",
        "\n",
        "        self.conv2 = nn.Conv1d(cnn_filters, cnn_filters * 2, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(cnn_filters * 2)\n",
        "        self.pool2 = nn.MaxPool1d(2)  # 384 â†’ 192\n",
        "\n",
        "        self.conv3 = nn.Conv1d(cnn_filters * 2, cnn_filters * 4, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(cnn_filters * 4)\n",
        "        self.pool3 = nn.MaxPool1d(2)  # 192 â†’ 96\n",
        "\n",
        "        self.conv4 = nn.Conv1d(cnn_filters * 4, cnn_filters * 8, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm1d(cnn_filters * 8)\n",
        "        self.pool4 = nn.MaxPool1d(2)  # 96 â†’ 48\n",
        "\n",
        "        self.cnn_out_dim = cnn_filters * 8 * 48  # = 1024 * 48\n",
        "\n",
        "        # LSTM Branch\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Updated Classifier: 256 â†’ 128 â†’ 64 â†’ 1\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.cnn_out_dim + lstm_hidden * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN Branch\n",
        "        x_cnn = x.unsqueeze(1)\n",
        "        x_cnn = self.pool1(torch.relu(self.bn1(self.conv1(x_cnn))))   # â†’ 384\n",
        "        x_cnn = self.pool2(torch.relu(self.bn2(self.conv2(x_cnn))))   # â†’ 192\n",
        "        x_cnn = self.pool3(torch.relu(self.bn3(self.conv3(x_cnn))))   # â†’ 96\n",
        "        x_cnn = self.pool4(torch.relu(self.bn4(self.conv4(x_cnn))))   # â†’ 48\n",
        "        x_cnn = x_cnn.view(x_cnn.size(0), -1)  # Flatten\n",
        "\n",
        "        # LSTM Branch\n",
        "        x_lstm = x.unsqueeze(1)\n",
        "        _, (hn, _) = self.lstm(x_lstm)\n",
        "        x_lstm = torch.cat((hn[-2], hn[-1]), dim=1)  # Final hidden states from both directions\n",
        "\n",
        "        # Combine\n",
        "        combined = torch.cat((x_cnn, x_lstm), dim=1)\n",
        "        return self.classifier(combined)"
      ],
      "metadata": {
        "id": "p6Clh8vC3k5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = CNN_LSTM_Parallel(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "LbxhXHlx5s5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 20 # ONLY FOR THIS\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8jv4HAT5xjn",
        "outputId": "12616738-3b67-406b-b7a4-5237ca68c75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 33.9789 - Val Loss: 8.4127\n",
            "Epoch 2/20 - Train Loss: 33.2603 - Val Loss: 8.2974\n",
            "Epoch 3/20 - Train Loss: 33.4433 - Val Loss: 8.5808\n",
            "Epoch 4/20 - Train Loss: 33.3177 - Val Loss: 8.2971\n",
            "Epoch 5/20 - Train Loss: 33.0556 - Val Loss: 8.1449\n",
            "Epoch 6/20 - Train Loss: 32.2941 - Val Loss: 7.7643\n",
            "Epoch 7/20 - Train Loss: 30.4453 - Val Loss: 7.5918\n",
            "Epoch 8/20 - Train Loss: 29.0451 - Val Loss: 7.2366\n",
            "Epoch 9/20 - Train Loss: 27.4162 - Val Loss: 6.9538\n",
            "Epoch 10/20 - Train Loss: 26.6458 - Val Loss: 6.6780\n",
            "Epoch 11/20 - Train Loss: 25.3993 - Val Loss: 6.7648\n",
            "Epoch 12/20 - Train Loss: 25.3052 - Val Loss: 6.8172\n",
            "Epoch 13/20 - Train Loss: 24.1146 - Val Loss: 6.6368\n",
            "Epoch 14/20 - Train Loss: 23.7647 - Val Loss: 7.2302\n",
            "Epoch 15/20 - Train Loss: 23.0633 - Val Loss: 6.9254\n",
            "Epoch 16/20 - Train Loss: 22.1138 - Val Loss: 7.4344\n",
            "Epoch 17/20 - Train Loss: 21.6630 - Val Loss: 6.4995\n",
            "Epoch 18/20 - Train Loss: 19.8870 - Val Loss: 7.6001\n",
            "Epoch 19/20 - Train Loss: 19.6312 - Val Loss: 7.2505\n",
            "Epoch 20/20 - Train Loss: 18.3656 - Val Loss: 7.3580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "print(\"\\nðŸ“Œ Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i_YaJmK51HR",
        "outputId": "434473d8-29e6-4c7a-a563-fde30c0df520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.6965\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8652    0.4266    0.5714       361\n",
            "         1.0     0.6449    0.9400    0.7650       400\n",
            "\n",
            "    accuracy                         0.6965       761\n",
            "   macro avg     0.7551    0.6833    0.6682       761\n",
            "weighted avg     0.7494    0.6965    0.6732       761\n",
            "\n",
            "\n",
            "ðŸ“Œ Confusion Matrix:\n",
            "[[154 207]\n",
            " [ 24 376]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_LSTM_Sequential(nn.Module):\n",
        "    def __init__(self, input_dim=768, cnn_filters=128, lstm_hidden=256, lstm_layers=3, dropout=0):\n",
        "        super(CNN_LSTM_Sequential, self).__init__()\n",
        "\n",
        "        # 4-layer CNN stack\n",
        "        self.conv1 = nn.Conv1d(1, cnn_filters, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm1d(cnn_filters)\n",
        "        self.pool1 = nn.MaxPool1d(2)  # 768 â†’ 384\n",
        "\n",
        "        self.conv2 = nn.Conv1d(cnn_filters, cnn_filters * 2, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm1d(cnn_filters * 2)\n",
        "        self.pool2 = nn.MaxPool1d(2)  # 384 â†’ 192\n",
        "\n",
        "        self.conv3 = nn.Conv1d(cnn_filters * 2, cnn_filters * 4, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(cnn_filters * 4)\n",
        "        self.pool3 = nn.MaxPool1d(2)  # 192 â†’ 96\n",
        "\n",
        "        self.conv4 = nn.Conv1d(cnn_filters * 4, cnn_filters * 8, kernel_size=3, padding=1)\n",
        "        self.bn4 = nn.BatchNorm1d(cnn_filters * 8)\n",
        "        self.pool4 = nn.MaxPool1d(2)  # 96 â†’ 48\n",
        "\n",
        "        self.lstm_input_dim = cnn_filters * 8  # 1024\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.lstm_input_dim,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=dropout if lstm_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Final Classifier: 256 â†’ 128 â†’ 64 â†’ 1\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # [B, 1, 768]\n",
        "\n",
        "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))  # â†’ 384\n",
        "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))  # â†’ 192\n",
        "        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))  # â†’ 96\n",
        "        x = self.pool4(torch.relu(self.bn4(self.conv4(x))))  # â†’ 48\n",
        "\n",
        "        x = x.permute(0, 2, 1)  # [B, 48, 1024]\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        x = torch.cat((hn[-2], hn[-1]), dim=1)  # [B, 512]\n",
        "\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "zuUsLl2H6Y_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = CNN_LSTM_Sequential(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "-gKHNB0M7Zys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40 # ONLY FOR THIS\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo-1E4u57yst",
        "outputId": "7d8af2bf-863d-4956-b763-1103e13c93d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.2521 - Val Loss: 8.3047\n",
            "Epoch 2/40 - Train Loss: 33.2334 - Val Loss: 8.3028\n",
            "Epoch 3/40 - Train Loss: 33.2197 - Val Loss: 8.3027\n",
            "Epoch 4/40 - Train Loss: 33.2238 - Val Loss: 8.3028\n",
            "Epoch 5/40 - Train Loss: 33.2139 - Val Loss: 8.3021\n",
            "Epoch 6/40 - Train Loss: 33.2197 - Val Loss: 8.3020\n",
            "Epoch 7/40 - Train Loss: 33.2147 - Val Loss: 8.3020\n",
            "Epoch 8/40 - Train Loss: 33.2191 - Val Loss: 8.3020\n",
            "Epoch 9/40 - Train Loss: 33.2095 - Val Loss: 8.3020\n",
            "Epoch 10/40 - Train Loss: 33.2109 - Val Loss: 8.3019\n",
            "Epoch 11/40 - Train Loss: 33.1998 - Val Loss: 8.3016\n",
            "Epoch 12/40 - Train Loss: 33.2217 - Val Loss: 8.3023\n",
            "Epoch 13/40 - Train Loss: 33.2186 - Val Loss: 8.3021\n",
            "Epoch 14/40 - Train Loss: 33.2170 - Val Loss: 8.3020\n",
            "Epoch 15/40 - Train Loss: 33.2134 - Val Loss: 8.3020\n",
            "Epoch 16/40 - Train Loss: 33.2122 - Val Loss: 8.3021\n",
            "Epoch 17/40 - Train Loss: 33.2161 - Val Loss: 8.3021\n",
            "Epoch 18/40 - Train Loss: 33.2035 - Val Loss: 8.3020\n",
            "Epoch 19/40 - Train Loss: 33.2080 - Val Loss: 8.3020\n",
            "Epoch 20/40 - Train Loss: 33.2138 - Val Loss: 8.3020\n",
            "Epoch 21/40 - Train Loss: 33.2057 - Val Loss: 8.3020\n",
            "Epoch 22/40 - Train Loss: 33.2075 - Val Loss: 8.3020\n",
            "Epoch 23/40 - Train Loss: 33.2117 - Val Loss: 8.3020\n",
            "Epoch 24/40 - Train Loss: 33.2136 - Val Loss: 8.3020\n",
            "Epoch 25/40 - Train Loss: 33.2092 - Val Loss: 8.3020\n",
            "Epoch 26/40 - Train Loss: 33.2093 - Val Loss: 8.3020\n",
            "Epoch 27/40 - Train Loss: 33.2133 - Val Loss: 8.3020\n",
            "Epoch 28/40 - Train Loss: 33.2136 - Val Loss: 8.3021\n",
            "Epoch 29/40 - Train Loss: 33.2142 - Val Loss: 8.3020\n",
            "Epoch 30/40 - Train Loss: 33.2074 - Val Loss: 8.3020\n",
            "Epoch 31/40 - Train Loss: 33.2180 - Val Loss: 8.3020\n",
            "Epoch 32/40 - Train Loss: 33.2147 - Val Loss: 8.3020\n",
            "Epoch 33/40 - Train Loss: 33.2085 - Val Loss: 8.3021\n",
            "Epoch 34/40 - Train Loss: 33.2131 - Val Loss: 8.3021\n",
            "Epoch 35/40 - Train Loss: 33.2096 - Val Loss: 8.3020\n",
            "Epoch 36/40 - Train Loss: 33.2136 - Val Loss: 8.3020\n",
            "Epoch 37/40 - Train Loss: 33.2082 - Val Loss: 8.3020\n",
            "Epoch 38/40 - Train Loss: 33.2065 - Val Loss: 8.3020\n",
            "Epoch 39/40 - Train Loss: 33.2191 - Val Loss: 8.3022\n",
            "Epoch 40/40 - Train Loss: 33.2058 - Val Loss: 8.3020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "print(\"\\nðŸ“Œ Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAXUCOrZ72bO",
        "outputId": "b0d6b881-cb59-4774-ff77-863aa773da54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.5256\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.0000    0.0000    0.0000       361\n",
            "         1.0     0.5256    1.0000    0.6891       400\n",
            "\n",
            "    accuracy                         0.5256       761\n",
            "   macro avg     0.2628    0.5000    0.3445       761\n",
            "weighted avg     0.2763    0.5256    0.3622       761\n",
            "\n",
            "\n",
            "ðŸ“Œ Confusion Matrix:\n",
            "[[  0 361]\n",
            " [  0 400]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. CNN_LSTM_Interleaved Model\n",
        "# ====================================\n",
        "class CNN_LSTM_Interleaved(nn.Module):\n",
        "    def __init__(self, input_dim=768, cnn_filters=128, lstm_hidden=256, lstm_layers=1):\n",
        "        super(CNN_LSTM_Interleaved, self).__init__()\n",
        "\n",
        "        # Block 1: CNN âž LSTM\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(1, cnn_filters, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_filters),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2)  # 768 â†’ 384\n",
        "        )\n",
        "        self.lstm1 = nn.LSTM(\n",
        "            input_size=cnn_filters,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Block 2: CNN âž LSTM\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv1d(lstm_hidden * 2, cnn_filters * 2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_filters * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2)  # 384 â†’ 192\n",
        "        )\n",
        "        self.lstm2 = nn.LSTM(\n",
        "            input_size=cnn_filters * 2,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Block 3: CNN âž LSTM\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv1d(lstm_hidden * 2, cnn_filters * 4, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(cnn_filters * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2)  # 192 â†’ 96\n",
        "        )\n",
        "        self.lstm3 = nn.LSTM(\n",
        "            input_size=cnn_filters * 4,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Final classifier: 256 â†’ 128 â†’ 64 â†’ 1\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # [B, 1, 768]\n",
        "\n",
        "        # Block 1\n",
        "        x = self.conv1(x)             # [B, 128, 384]\n",
        "        x = x.permute(0, 2, 1)        # [B, 384, 128]\n",
        "        x, _ = self.lstm1(x)          # [B, 384, 512]\n",
        "\n",
        "        # Block 2\n",
        "        x = x.permute(0, 2, 1)        # [B, 512, 384]\n",
        "        x = self.conv2(x)             # [B, 256, 192]\n",
        "        x = x.permute(0, 2, 1)        # [B, 192, 256]\n",
        "        x, _ = self.lstm2(x)          # [B, 192, 512]\n",
        "\n",
        "        # Block 3\n",
        "        x = x.permute(0, 2, 1)        # [B, 512, 192]\n",
        "        x = self.conv3(x)             # [B, 512, 96]\n",
        "        x = x.permute(0, 2, 1)        # [B, 96, 512]\n",
        "        x, _ = self.lstm3(x)          # [B, 96, 512]\n",
        "\n",
        "        # Final time step\n",
        "        x = x[:, -1, :]               # [B, 512]\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "g5sLfbrd7-TR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = CNN_LSTM_Interleaved(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "Z_eqQjVJ9Ws_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40 # ONLY FOR THIS\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4caimvd9rHK",
        "outputId": "dc7a6482-edec-451c-ad50-a3c7d53a030e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.2103 - Val Loss: 8.3374\n",
            "Epoch 2/40 - Train Loss: 32.9761 - Val Loss: 8.2596\n",
            "Epoch 3/40 - Train Loss: 32.7461 - Val Loss: 8.1754\n",
            "Epoch 4/40 - Train Loss: 32.7339 - Val Loss: 8.4061\n",
            "Epoch 5/40 - Train Loss: 32.9776 - Val Loss: 8.1798\n",
            "Epoch 6/40 - Train Loss: 32.6052 - Val Loss: 8.1765\n",
            "Epoch 7/40 - Train Loss: 32.7848 - Val Loss: 8.3322\n",
            "Epoch 8/40 - Train Loss: 32.7193 - Val Loss: 8.0610\n",
            "Epoch 9/40 - Train Loss: 31.9152 - Val Loss: 8.1054\n",
            "Epoch 10/40 - Train Loss: 31.9986 - Val Loss: 8.0480\n",
            "Epoch 11/40 - Train Loss: 31.8427 - Val Loss: 8.0547\n",
            "Epoch 12/40 - Train Loss: 31.2468 - Val Loss: 7.8379\n",
            "Epoch 13/40 - Train Loss: 31.2627 - Val Loss: 7.9126\n",
            "Epoch 14/40 - Train Loss: 30.8018 - Val Loss: 7.8513\n",
            "Epoch 15/40 - Train Loss: 30.4690 - Val Loss: 7.7678\n",
            "Epoch 16/40 - Train Loss: 30.1293 - Val Loss: 7.7591\n",
            "Epoch 17/40 - Train Loss: 30.1292 - Val Loss: 7.7481\n",
            "Epoch 18/40 - Train Loss: 29.7472 - Val Loss: 7.6699\n",
            "Epoch 19/40 - Train Loss: 29.6153 - Val Loss: 7.7614\n",
            "Epoch 20/40 - Train Loss: 29.1337 - Val Loss: 7.4695\n",
            "Epoch 21/40 - Train Loss: 28.7148 - Val Loss: 7.3158\n",
            "Epoch 22/40 - Train Loss: 29.3398 - Val Loss: 7.9573\n",
            "Epoch 23/40 - Train Loss: 28.6964 - Val Loss: 7.2690\n",
            "Epoch 24/40 - Train Loss: 29.2179 - Val Loss: 7.3694\n",
            "Epoch 25/40 - Train Loss: 27.9915 - Val Loss: 7.3340\n",
            "Epoch 26/40 - Train Loss: 27.7177 - Val Loss: 7.2987\n",
            "Epoch 27/40 - Train Loss: 27.3126 - Val Loss: 7.0973\n",
            "Epoch 28/40 - Train Loss: 27.0069 - Val Loss: 9.3427\n",
            "Epoch 29/40 - Train Loss: 27.2452 - Val Loss: 7.2004\n",
            "Epoch 30/40 - Train Loss: 26.7255 - Val Loss: 7.8850\n",
            "Epoch 31/40 - Train Loss: 26.3462 - Val Loss: 7.2975\n",
            "Epoch 32/40 - Train Loss: 25.3650 - Val Loss: 6.9013\n",
            "Epoch 33/40 - Train Loss: 24.5856 - Val Loss: 7.1709\n",
            "Epoch 34/40 - Train Loss: 24.8043 - Val Loss: 7.2985\n",
            "Epoch 35/40 - Train Loss: 23.6332 - Val Loss: 7.1803\n",
            "Epoch 36/40 - Train Loss: 23.3251 - Val Loss: 7.6122\n",
            "Epoch 37/40 - Train Loss: 21.9624 - Val Loss: 8.1341\n",
            "Epoch 38/40 - Train Loss: 21.6999 - Val Loss: 8.5081\n",
            "Epoch 39/40 - Train Loss: 22.0701 - Val Loss: 8.8008\n",
            "Epoch 40/40 - Train Loss: 20.4606 - Val Loss: 7.8657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "print(\"\\nðŸ“Œ Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD3cnK_a9tmg",
        "outputId": "1e37c398-dff9-462b-9d0e-873bde2baf26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.6649\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.6183    0.7673    0.6848       361\n",
            "         1.0     0.7316    0.5725    0.6424       400\n",
            "\n",
            "    accuracy                         0.6649       761\n",
            "   macro avg     0.6750    0.6699    0.6636       761\n",
            "weighted avg     0.6779    0.6649    0.6625       761\n",
            "\n",
            "\n",
            "ðŸ“Œ Confusion Matrix:\n",
            "[[277  84]\n",
            " [171 229]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANN"
      ],
      "metadata": {
        "id": "DSmftcEgNsrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. ANN Model\n",
        "# ====================================\n",
        "class ANNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ANNClassifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "7sSkd05f_6IR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = ANNClassifier(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "ewNEyTUgNyX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40 # ONLY FOR THIS\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGsZXjz-N3KZ",
        "outputId": "79377de0-7a62-4e6d-8df2-f48d572a96d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.2473 - Val Loss: 8.2382\n",
            "Epoch 2/40 - Train Loss: 30.4616 - Val Loss: 6.7491\n",
            "Epoch 3/40 - Train Loss: 27.5800 - Val Loss: 6.6576\n",
            "Epoch 4/40 - Train Loss: 26.6932 - Val Loss: 6.6085\n",
            "Epoch 5/40 - Train Loss: 25.8687 - Val Loss: 6.4112\n",
            "Epoch 6/40 - Train Loss: 25.9460 - Val Loss: 6.4376\n",
            "Epoch 7/40 - Train Loss: 25.2993 - Val Loss: 6.5985\n",
            "Epoch 8/40 - Train Loss: 24.2964 - Val Loss: 6.3229\n",
            "Epoch 9/40 - Train Loss: 24.0264 - Val Loss: 6.6458\n",
            "Epoch 10/40 - Train Loss: 23.9917 - Val Loss: 6.6969\n",
            "Epoch 11/40 - Train Loss: 24.8373 - Val Loss: 6.2256\n",
            "Epoch 12/40 - Train Loss: 23.4469 - Val Loss: 6.1462\n",
            "Epoch 13/40 - Train Loss: 23.5481 - Val Loss: 6.0887\n",
            "Epoch 14/40 - Train Loss: 23.9560 - Val Loss: 6.7913\n",
            "Epoch 15/40 - Train Loss: 23.2264 - Val Loss: 6.1710\n",
            "Epoch 16/40 - Train Loss: 22.4554 - Val Loss: 6.2177\n",
            "Epoch 17/40 - Train Loss: 23.1373 - Val Loss: 5.9823\n",
            "Epoch 18/40 - Train Loss: 22.4928 - Val Loss: 6.1450\n",
            "Epoch 19/40 - Train Loss: 22.4776 - Val Loss: 6.2277\n",
            "Epoch 20/40 - Train Loss: 22.4381 - Val Loss: 6.0308\n",
            "Epoch 21/40 - Train Loss: 22.6127 - Val Loss: 6.2045\n",
            "Epoch 22/40 - Train Loss: 21.2532 - Val Loss: 6.2875\n",
            "Epoch 23/40 - Train Loss: 21.3684 - Val Loss: 6.3834\n",
            "Epoch 24/40 - Train Loss: 20.6840 - Val Loss: 6.1287\n",
            "Epoch 25/40 - Train Loss: 20.3414 - Val Loss: 5.9863\n",
            "Epoch 26/40 - Train Loss: 20.2207 - Val Loss: 6.2171\n",
            "Epoch 27/40 - Train Loss: 19.7151 - Val Loss: 6.4775\n",
            "Epoch 28/40 - Train Loss: 19.7191 - Val Loss: 6.5098\n",
            "Epoch 29/40 - Train Loss: 19.2008 - Val Loss: 6.0986\n",
            "Epoch 30/40 - Train Loss: 19.1077 - Val Loss: 6.5486\n",
            "Epoch 31/40 - Train Loss: 18.5424 - Val Loss: 6.9918\n",
            "Epoch 32/40 - Train Loss: 19.4274 - Val Loss: 6.5979\n",
            "Epoch 33/40 - Train Loss: 18.3064 - Val Loss: 6.0258\n",
            "Epoch 34/40 - Train Loss: 18.5864 - Val Loss: 6.3671\n",
            "Epoch 35/40 - Train Loss: 17.9839 - Val Loss: 6.1488\n",
            "Epoch 36/40 - Train Loss: 17.5186 - Val Loss: 6.6029\n",
            "Epoch 37/40 - Train Loss: 16.5977 - Val Loss: 6.8930\n",
            "Epoch 38/40 - Train Loss: 16.0543 - Val Loss: 6.7146\n",
            "Epoch 39/40 - Train Loss: 16.3087 - Val Loss: 6.9286\n",
            "Epoch 40/40 - Train Loss: 15.6541 - Val Loss: 6.6415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "print(\"\\nðŸ“Œ Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3iYgyndOAbN",
        "outputId": "194edeef-4f52-4012-8959-e34ed10c1f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7635\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7401    0.7729    0.7561       361\n",
            "         1.0     0.7865    0.7550    0.7704       400\n",
            "\n",
            "    accuracy                         0.7635       761\n",
            "   macro avg     0.7633    0.7639    0.7633       761\n",
            "weighted avg     0.7644    0.7635    0.7636       761\n",
            "\n",
            "\n",
            "ðŸ“Œ Confusion Matrix:\n",
            "[[279  82]\n",
            " [ 98 302]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 4. More Complex ANN Model\n",
        "# ====================================\n",
        "class ANNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ANNClassifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "QSaKMYu3ODzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 5. Model, Loss, Optimizer Setup\n",
        "# ====================================\n",
        "model = ANNClassifier(input_dim=768).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "-hqrX6XjOnRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 6. Training Loop\n",
        "# ====================================\n",
        "epochs = 40 # ONLY FOR THIS\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # === Validation Phase ===\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfbmkVqVOXqX",
        "outputId": "a947545f-bdc3-4da4-afd3-6b602d198646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40 - Train Loss: 33.0238 - Val Loss: 7.8168\n",
            "Epoch 2/40 - Train Loss: 28.5069 - Val Loss: 7.2364\n",
            "Epoch 3/40 - Train Loss: 28.0093 - Val Loss: 6.4448\n",
            "Epoch 4/40 - Train Loss: 27.1042 - Val Loss: 6.7354\n",
            "Epoch 5/40 - Train Loss: 25.2498 - Val Loss: 6.3083\n",
            "Epoch 6/40 - Train Loss: 25.4209 - Val Loss: 6.7748\n",
            "Epoch 7/40 - Train Loss: 24.7428 - Val Loss: 6.3939\n",
            "Epoch 8/40 - Train Loss: 24.3863 - Val Loss: 6.2417\n",
            "Epoch 9/40 - Train Loss: 24.2622 - Val Loss: 6.1913\n",
            "Epoch 10/40 - Train Loss: 23.7248 - Val Loss: 6.1802\n",
            "Epoch 11/40 - Train Loss: 24.1460 - Val Loss: 6.1745\n",
            "Epoch 12/40 - Train Loss: 23.3026 - Val Loss: 6.2439\n",
            "Epoch 13/40 - Train Loss: 23.5605 - Val Loss: 6.1849\n",
            "Epoch 14/40 - Train Loss: 22.7691 - Val Loss: 6.0616\n",
            "Epoch 15/40 - Train Loss: 22.4485 - Val Loss: 6.1224\n",
            "Epoch 16/40 - Train Loss: 22.4688 - Val Loss: 6.3699\n",
            "Epoch 17/40 - Train Loss: 22.6131 - Val Loss: 6.2875\n",
            "Epoch 18/40 - Train Loss: 23.2420 - Val Loss: 7.0796\n",
            "Epoch 19/40 - Train Loss: 22.1500 - Val Loss: 6.0742\n",
            "Epoch 20/40 - Train Loss: 22.3805 - Val Loss: 6.1617\n",
            "Epoch 21/40 - Train Loss: 20.5262 - Val Loss: 6.2520\n",
            "Epoch 22/40 - Train Loss: 20.5992 - Val Loss: 6.1323\n",
            "Epoch 23/40 - Train Loss: 21.5677 - Val Loss: 6.2844\n",
            "Epoch 24/40 - Train Loss: 20.5438 - Val Loss: 6.2651\n",
            "Epoch 25/40 - Train Loss: 20.6186 - Val Loss: 6.0576\n",
            "Epoch 26/40 - Train Loss: 19.7627 - Val Loss: 6.2694\n",
            "Epoch 27/40 - Train Loss: 19.3484 - Val Loss: 6.5335\n",
            "Epoch 28/40 - Train Loss: 17.9113 - Val Loss: 7.7896\n",
            "Epoch 29/40 - Train Loss: 19.9885 - Val Loss: 5.9875\n",
            "Epoch 30/40 - Train Loss: 18.2017 - Val Loss: 6.9667\n",
            "Epoch 31/40 - Train Loss: 20.3499 - Val Loss: 7.8333\n",
            "Epoch 32/40 - Train Loss: 17.3299 - Val Loss: 6.8517\n",
            "Epoch 33/40 - Train Loss: 18.9980 - Val Loss: 7.1060\n",
            "Epoch 34/40 - Train Loss: 18.0693 - Val Loss: 6.2465\n",
            "Epoch 35/40 - Train Loss: 16.7754 - Val Loss: 6.6067\n",
            "Epoch 36/40 - Train Loss: 16.8950 - Val Loss: 7.9018\n",
            "Epoch 37/40 - Train Loss: 16.6565 - Val Loss: 6.6058\n",
            "Epoch 38/40 - Train Loss: 17.2860 - Val Loss: 6.1935\n",
            "Epoch 39/40 - Train Loss: 15.4492 - Val Loss: 6.8309\n",
            "Epoch 40/40 - Train Loss: 16.5479 - Val Loss: 6.0221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 7. Evaluation\n",
        "# ====================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        logits = model(X_batch)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_preds.extend(preds.flatten())\n",
        "        all_labels.extend(y_batch.numpy().flatten())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nâœ… Test Accuracy: {acc:.4f}\")\n",
        "print(\"\\nðŸ“Š Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "print(\"\\nðŸ“Œ Confusion Matrix:\")\n",
        "print(confusion_matrix(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO6kGlFCObZr",
        "outputId": "c03530a8-83e5-4463-cdda-db4506b30e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Test Accuracy: 0.7622\n",
            "\n",
            "ðŸ“Š Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7123    0.8366    0.7694       361\n",
            "         1.0     0.8249    0.6950    0.7544       400\n",
            "\n",
            "    accuracy                         0.7622       761\n",
            "   macro avg     0.7686    0.7658    0.7619       761\n",
            "weighted avg     0.7715    0.7622    0.7615       761\n",
            "\n",
            "\n",
            "ðŸ“Œ Confusion Matrix:\n",
            "[[302  59]\n",
            " [122 278]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining Deep Learning Models does not improve accuracy."
      ],
      "metadata": {
        "id": "A7HSJyouK_2V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2wbF-v6vOevx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}